{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/arxiv-metadata-oai-snapshot-10000.csv'\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pages(s):\n",
    "    match = re.search(r\"(\\d+)\\s*pages\", s)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>journal-ref</th>\n",
       "      <th>doi</th>\n",
       "      <th>report-no</th>\n",
       "      <th>categories</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors_parsed</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704.0001</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>37 pages, 15 figures; published version</td>\n",
       "      <td>Phys.Rev.D76:013009,2007</td>\n",
       "      <td>10.1103/PhysRevD.76.013009</td>\n",
       "      <td>ANL-HEP-PR-07-12</td>\n",
       "      <td>[hep-ph]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A fully differential calculation in perturba...</td>\n",
       "      <td>[Balázs C., Berger E. L., Nadolsky P. M., Yuan...</td>\n",
       "      <td>1.175542e+09</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704.0002</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>To appear in Graphs and Combinatorics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[math.CO, cs.CG]</td>\n",
       "      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-...</td>\n",
       "      <td>[Streinu Ileana, Theran Louis]</td>\n",
       "      <td>1.175308e+09</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>704.0003</td>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "      <td>23 pages, 3 figures</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[physics.gen-ph]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The evolution of Earth-Moon system is descri...</td>\n",
       "      <td>[Pan Hongjun]</td>\n",
       "      <td>1.175460e+09</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>704.0004</td>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "      <td>11 pages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[math.CO]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We show that a determinant of Stirling cycle...</td>\n",
       "      <td>[Callan David]</td>\n",
       "      <td>1.175311e+09</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>704.0005</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Illinois J. Math. 52 (2008) no.2, 681-689</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[math.CA, math.FA]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In this paper we show how to compute the $\\L...</td>\n",
       "      <td>[Abu-Shammala Wael, Torchinsky Alberto]</td>\n",
       "      <td>1.175537e+09</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "0  704.0001  Calculation of prompt diphoton production cros...   \n",
       "1  704.0002           Sparsity-certifying Graph Decompositions   \n",
       "2  704.0003  The evolution of the Earth-Moon system based o...   \n",
       "3  704.0004  A determinant of Stirling cycle numbers counts...   \n",
       "4  704.0005  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
       "\n",
       "                                  comments  \\\n",
       "0  37 pages, 15 figures; published version   \n",
       "1    To appear in Graphs and Combinatorics   \n",
       "2                      23 pages, 3 figures   \n",
       "3                                 11 pages   \n",
       "4                                      NaN   \n",
       "\n",
       "                                 journal-ref                         doi  \\\n",
       "0                   Phys.Rev.D76:013009,2007  10.1103/PhysRevD.76.013009   \n",
       "1                                        NaN                         NaN   \n",
       "2                                        NaN                         NaN   \n",
       "3                                        NaN                         NaN   \n",
       "4  Illinois J. Math. 52 (2008) no.2, 681-689                         NaN   \n",
       "\n",
       "          report-no          categories  \\\n",
       "0  ANL-HEP-PR-07-12            [hep-ph]   \n",
       "1               NaN    [math.CO, cs.CG]   \n",
       "2               NaN    [physics.gen-ph]   \n",
       "3               NaN           [math.CO]   \n",
       "4               NaN  [math.CA, math.FA]   \n",
       "\n",
       "                                             license  \\\n",
       "0                                                NaN   \n",
       "1  http://arxiv.org/licenses/nonexclusive-distrib...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                            abstract  \\\n",
       "0    A fully differential calculation in perturba...   \n",
       "1    We describe a new algorithm, the $(k,\\ell)$-...   \n",
       "2    The evolution of Earth-Moon system is descri...   \n",
       "3    We show that a determinant of Stirling cycle...   \n",
       "4    In this paper we show how to compute the $\\L...   \n",
       "\n",
       "                                      authors_parsed     timestamp  pages  \n",
       "0  [Balázs C., Berger E. L., Nadolsky P. M., Yuan...  1.175542e+09   37.0  \n",
       "1                     [Streinu Ileana, Theran Louis]  1.175308e+09    NaN  \n",
       "2                                      [Pan Hongjun]  1.175460e+09   23.0  \n",
       "3                                     [Callan David]  1.175311e+09   11.0  \n",
       "4            [Abu-Shammala Wael, Torchinsky Alberto]  1.175537e+09    NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['authors_parsed'] = df['authors_parsed'].apply(lambda x: [\" \".join(i).strip() for i in eval(x)])\n",
    "df['versions'] = df['versions'].apply(lambda x: eval(x)[0][\"created\"])\n",
    "df['timestamp'] = pd.to_datetime(df['versions'], format=\"%a, %d %b %Y %H:%M:%S %Z\")\n",
    "df['timestamp'] = df['timestamp'].apply(lambda x: x.timestamp())\n",
    "df[\"categories\"] = df[\"categories\"].apply(lambda x: x.split(\" \"))\n",
    "df.drop(columns=[\"submitter\", \"versions\", \"update_date\", \"authors\"], inplace=True)\n",
    "df[\"pages\"] = df.comments.apply(lambda x: extract_pages(str(x)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_normalize(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    return [token.text.lower() for token in doc if not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short = df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['  ', 'a', 'fully', 'differential', 'calculation', 'in', 'perturbative', 'quantum', 'chromodynamics', 'is', '\\r\\n', 'presented', 'for', 'the', 'production', 'of', 'massive', 'photon', 'pairs', 'at', 'hadron', 'colliders', 'all', '\\r\\n', 'next', 'to', 'leading', 'order', 'perturbative', 'contributions', 'from', 'quark', 'antiquark', '\\r\\n', 'gluon-(anti)quark', 'and', 'gluon', 'gluon', 'subprocesses', 'are', 'included', 'as', 'well', 'as', '\\r\\n', 'all', 'orders', 'resummation', 'of', 'initial', 'state', 'gluon', 'radiation', 'valid', 'at', '\\r\\n', 'next', 'to', 'next', 'to', 'leading', 'logarithmic', 'accuracy', 'the', 'region', 'of', 'phase', 'space', 'is', '\\r\\n', 'specified', 'in', 'which', 'the', 'calculation', 'is', 'most', 'reliable', 'good', 'agreement', 'is', '\\r\\n', 'demonstrated', 'with', 'data', 'from', 'the', 'fermilab', 'tevatron', 'and', 'predictions', 'are', 'made', 'for', '\\r\\n', 'more', 'detailed', 'tests', 'with', 'cdf', 'and', 'do', 'data', 'predictions', 'are', 'shown', 'for', '\\r\\n', 'distributions', 'of', 'diphoton', 'pairs', 'produced', 'at', 'the', 'energy', 'of', 'the', 'large', 'hadron', '\\r\\n', 'collider', 'lhc', 'distributions', 'of', 'the', 'diphoton', 'pairs', 'from', 'the', 'decay', 'of', 'a', 'higgs', '\\r\\n', 'boson', 'are', 'contrasted', 'with', 'those', 'produced', 'from', 'qcd', 'processes', 'at', 'the', 'lhc', 'showing', '\\r\\n', 'that', 'enhanced', 'sensitivity', 'to', 'the', 'signal', 'can', 'be', 'obtained', 'with', 'judicious', '\\r\\n', 'selection', 'of', 'events', '\\r\\n'], ['  ', 'we', 'describe', 'a', 'new', 'algorithm', 'the', '$', 'k,\\\\ell)$-pebble', 'game', 'with', 'colors', 'and', 'use', '\\r\\n', 'it', 'obtain', 'a', 'characterization', 'of', 'the', 'family', 'of', '$', 'k,\\\\ell)$-sparse', 'graphs', 'and', '\\r\\n', 'algorithmic', 'solutions', 'to', 'a', 'family', 'of', 'problems', 'concerning', 'tree', 'decompositions', 'of', '\\r\\n', 'graphs', 'special', 'instances', 'of', 'sparse', 'graphs', 'appear', 'in', 'rigidity', 'theory', 'and', 'have', '\\r\\n', 'received', 'increased', 'attention', 'in', 'recent', 'years', 'in', 'particular', 'our', 'colored', '\\r\\n', 'pebbles', 'generalize', 'and', 'strengthen', 'the', 'previous', 'results', 'of', 'lee', 'and', 'streinu', 'and', '\\r\\n', 'give', 'a', 'new', 'proof', 'of', 'the', 'tutte', 'nash', 'williams', 'characterization', 'of', 'arboricity', 'we', '\\r\\n', 'also', 'present', 'a', 'new', 'decomposition', 'that', 'certifies', 'sparsity', 'based', 'on', 'the', '\\r\\n', '$', 'k,\\\\ell)$-pebble', 'game', 'with', 'colors', 'our', 'work', 'also', 'exposes', 'connections', 'between', '\\r\\n', 'pebble', 'game', 'algorithms', 'and', 'previous', 'sparse', 'graph', 'algorithms', 'by', 'gabow', 'gabow', 'and', '\\r\\n', 'westermann', 'and', 'hendrickson', '\\r\\n'], ['  ', 'the', 'evolution', 'of', 'earth', 'moon', 'system', 'is', 'described', 'by', 'the', 'dark', 'matter', 'field', '\\r\\n', 'fluid', 'model', 'proposed', 'in', 'the', 'meeting', 'of', 'division', 'of', 'particle', 'and', 'field', '2004', '\\r\\n', 'american', 'physical', 'society', 'the', 'current', 'behavior', 'of', 'the', 'earth', 'moon', 'system', 'agrees', '\\r\\n', 'with', 'this', 'model', 'very', 'well', 'and', 'the', 'general', 'pattern', 'of', 'the', 'evolution', 'of', 'the', '\\r\\n', 'moon', 'earth', 'system', 'described', 'by', 'this', 'model', 'agrees', 'with', 'geological', 'and', 'fossil', '\\r\\n', 'evidence', 'the', 'closest', 'distance', 'of', 'the', 'moon', 'to', 'earth', 'was', 'about', '259000', 'km', 'at', '4.5', '\\r\\n', 'billion', 'years', 'ago', 'which', 'is', 'far', 'beyond', 'the', 'roche', \"'s\", 'limit', 'the', 'result', 'suggests', '\\r\\n', 'that', 'the', 'tidal', 'friction', 'may', 'not', 'be', 'the', 'primary', 'cause', 'for', 'the', 'evolution', 'of', 'the', '\\r\\n', 'earth', 'moon', 'system', 'the', 'average', 'dark', 'matter', 'field', 'fluid', 'constant', 'derived', 'from', '\\r\\n', 'earth', 'moon', 'system', 'data', 'is', '4.39', 'x', '10^(-22', 's^(-1)m^(-1', 'this', 'model', 'predicts', '\\r\\n', 'that', 'the', 'mars', \"'s\", 'rotation', 'is', 'also', 'slowing', 'with', 'the', 'angular', 'acceleration', 'rate', '\\r\\n', 'about', '-4.38', 'x', '10^(-22', 'rad', 's^(-2', '\\r\\n'], ['  ', 'we', 'show', 'that', 'a', 'determinant', 'of', 'stirling', 'cycle', 'numbers', 'counts', 'unlabeled', 'acyclic', '\\r\\n', 'single', 'source', 'automata', 'the', 'proof', 'involves', 'a', 'bijection', 'from', 'these', 'automata', 'to', '\\r\\n', 'certain', 'marked', 'lattice', 'paths', 'and', 'a', 'sign', 'reversing', 'involution', 'to', 'evaluate', 'the', '\\r\\n', 'determinant', '\\r\\n'], ['  ', 'in', 'this', 'paper', 'we', 'show', 'how', 'to', 'compute', 'the', '$', '\\\\lambda_{\\\\alpha}$', 'norm', '$', '\\\\alpha\\\\ge', '\\r\\n', '0', '$', 'using', 'the', 'dyadic', 'grid', 'this', 'result', 'is', 'a', 'consequence', 'of', 'the', 'description', 'of', '\\r\\n', 'the', 'hardy', 'spaces', '$', 'h^p(r^n)$', 'in', 'terms', 'of', 'dyadic', 'and', 'special', 'atoms', '\\r\\n'], ['  ', 'we', 'study', 'the', 'two', 'particle', 'wave', 'function', 'of', 'paired', 'atoms', 'in', 'a', 'fermi', 'gas', 'with', '\\r\\n', 'tunable', 'interaction', 'strengths', 'controlled', 'by', 'feshbach', 'resonance', 'the', 'cooper', 'pair', '\\r\\n', 'wave', 'function', 'is', 'examined', 'for', 'its', 'bosonic', 'characters', 'which', 'is', 'quantified', 'by', '\\r\\n', 'the', 'correction', 'of', 'bose', 'enhancement', 'factor', 'associated', 'with', 'the', 'creation', 'and', '\\r\\n', 'annihilation', 'composite', 'particle', 'operators', 'an', 'example', 'is', 'given', 'for', 'a', '\\r\\n', 'three', 'dimensional', 'uniform', 'gas', 'two', 'definitions', 'of', 'cooper', 'pair', 'wave', 'function', 'are', '\\r\\n', 'examined', 'one', 'of', 'which', 'is', 'chosen', 'to', 'reflect', 'the', 'off', 'diagonal', 'long', 'range', 'order', '\\r\\n', 'odlro', 'another', 'one', 'corresponds', 'to', 'a', 'pair', 'projection', 'of', 'a', 'bcs', 'state', 'on', 'the', '\\r\\n', 'side', 'with', 'negative', 'scattering', 'length', 'we', 'found', 'that', 'paired', 'atoms', 'described', 'by', '\\r\\n', 'odlro', 'are', 'more', 'bosonic', 'than', 'the', 'pair', 'projected', 'definition', 'it', 'is', 'also', 'found', '\\r\\n', 'that', 'at', '$', 'k_f', 'a)^{-1', '\\\\ge', '1', '$', 'both', 'definitions', 'give', 'similar', 'results', 'where', 'more', '\\r\\n', 'than', '90', 'of', 'the', 'atoms', 'occupy', 'the', 'corresponding', 'molecular', 'condensates', '\\r\\n'], ['  ', 'a', 'rather', 'non', 'standard', 'quantum', 'representation', 'of', 'the', 'canonical', 'commutation', '\\r\\n', 'relations', 'of', 'quantum', 'mechanics', 'systems', 'known', 'as', 'the', 'polymer', 'representation', 'has', '\\r\\n', 'gained', 'some', 'attention', 'in', 'recent', 'years', 'due', 'to', 'its', 'possible', 'relation', 'with', 'planck', '\\r\\n', 'scale', 'physics', 'in', 'particular', 'this', 'approach', 'has', 'been', 'followed', 'in', 'a', 'symmetric', '\\r\\n', 'sector', 'of', 'loop', 'quantum', 'gravity', 'known', 'as', 'loop', 'quantum', 'cosmology', 'here', 'we', 'explore', '\\r\\n', 'different', 'aspects', 'of', 'the', 'relation', 'between', 'the', 'ordinary', 'schroedinger', 'theory', 'and', '\\r\\n', 'the', 'polymer', 'description', 'the', 'paper', 'has', 'two', 'parts', 'in', 'the', 'first', 'one', 'we', 'derive', '\\r\\n', 'the', 'polymer', 'quantum', 'mechanics', 'starting', 'from', 'the', 'ordinary', 'schroedinger', 'theory', '\\r\\n', 'and', 'show', 'that', 'the', 'polymer', 'description', 'arises', 'as', 'an', 'appropriate', 'limit', 'in', 'the', '\\r\\n', 'second', 'part', 'we', 'consider', 'the', 'continuum', 'limit', 'of', 'this', 'theory', 'namely', 'the', 'reverse', '\\r\\n', 'process', 'in', 'which', 'one', 'starts', 'from', 'the', 'discrete', 'theory', 'and', 'tries', 'to', 'recover', 'back', '\\r\\n', 'the', 'ordinary', 'schroedinger', 'quantum', 'mechanics', 'we', 'consider', 'several', 'examples', 'of', '\\r\\n', 'interest', 'including', 'the', 'harmonic', 'oscillator', 'the', 'free', 'particle', 'and', 'a', 'simple', '\\r\\n', 'cosmological', 'model', '\\r\\n'], ['  ', 'a', 'general', 'formulation', 'was', 'developed', 'to', 'represent', 'material', 'models', 'for', '\\r\\n', 'applications', 'in', 'dynamic', 'loading', 'numerical', 'methods', 'were', 'devised', 'to', 'calculate', '\\r\\n', 'response', 'to', 'shock', 'and', 'ramp', 'compression', 'and', 'ramp', 'decompression', 'generalizing', '\\r\\n', 'previous', 'solutions', 'for', 'scalar', 'equations', 'of', 'state', 'the', 'numerical', 'methods', 'were', '\\r\\n', 'found', 'to', 'be', 'flexible', 'and', 'robust', 'and', 'matched', 'analytic', 'results', 'to', 'a', 'high', '\\r\\n', 'accuracy', 'the', 'basic', 'ramp', 'and', 'shock', 'solution', 'methods', 'were', 'coupled', 'to', 'solve', 'for', '\\r\\n', 'composite', 'deformation', 'paths', 'such', 'as', 'shock', 'induced', 'impacts', 'and', 'shock', '\\r\\n', 'interactions', 'with', 'a', 'planar', 'interface', 'between', 'different', 'materials', 'these', '\\r\\n', 'calculations', 'capture', 'much', 'of', 'the', 'physics', 'of', 'typical', 'material', 'dynamics', '\\r\\n', 'experiments', 'without', 'requiring', 'spatially', 'resolving', 'simulations', 'example', '\\r\\n', 'calculations', 'were', 'made', 'of', 'loading', 'histories', 'in', 'metals', 'illustrating', 'the', 'effects', '\\r\\n', 'of', 'plastic', 'work', 'on', 'the', 'temperatures', 'induced', 'in', 'quasi', 'isentropic', 'and', '\\r\\n', 'shock', 'release', 'experiments', 'and', 'the', 'effect', 'of', 'a', 'phase', 'transition', '\\r\\n'], ['  ', 'we', 'discuss', 'the', 'results', 'from', 'the', 'combined', 'irac', 'and', 'mips', 'c2d', 'spitzer', 'legacy', '\\r\\n', 'observations', 'of', 'the', 'serpens', 'star', 'forming', 'region', 'in', 'particular', 'we', 'present', 'a', 'set', '\\r\\n', 'of', 'criteria', 'for', 'isolating', 'bona', 'fide', 'young', 'stellar', 'objects', 'yso', \"'s\", 'from', 'the', '\\r\\n', 'extensive', 'background', 'contamination', 'by', 'extra', 'galactic', 'objects', 'we', 'then', 'discuss', '\\r\\n', 'the', 'properties', 'of', 'the', 'resulting', 'high', 'confidence', 'set', 'of', 'yso', \"'s\", 'we', 'find', '235', 'such', '\\r\\n', 'objects', 'in', 'the', '0.85', 'deg^2', 'field', 'that', 'was', 'covered', 'with', 'both', 'irac', 'and', 'mips', 'an', '\\r\\n', 'additional', 'set', 'of', '51', 'lower', 'confidence', 'yso', \"'s\", 'outside', 'this', 'area', 'is', 'identified', '\\r\\n', 'from', 'the', 'mips', 'data', 'combined', 'with', '2mass', 'photometry', 'we', 'describe', 'two', 'sets', 'of', '\\r\\n', 'results', 'color', 'color', 'diagrams', 'to', 'compare', 'our', 'observed', 'source', 'properties', 'with', '\\r\\n', 'those', 'of', 'theoretical', 'models', 'for', 'star', 'disk', 'envelope', 'systems', 'and', 'our', 'own', 'modeling', '\\r\\n', 'of', 'the', 'subset', 'of', 'our', 'objects', 'that', 'appear', 'to', 'be', 'star+disks', 'these', 'objects', '\\r\\n', 'exhibit', 'a', 'very', 'wide', 'range', 'of', 'disk', 'properties', 'from', 'many', 'that', 'can', 'be', 'fit', 'with', '\\r\\n', 'actively', 'accreting', 'disks', 'to', 'some', 'with', 'both', 'passive', 'disks', 'and', 'even', 'possibly', '\\r\\n', 'debris', 'disks', 'we', 'find', 'that', 'the', 'luminosity', 'function', 'of', 'yso', \"'s\", 'in', 'serpens', 'extends', '\\r\\n', 'down', 'to', 'at', 'least', 'a', 'few', 'x', '.001', 'lsun', 'or', 'lower', 'for', 'an', 'assumed', 'distance', 'of', '260', 'pc', '\\r\\n', 'the', 'lower', 'limit', 'may', 'be', 'set', 'by', 'our', 'inability', 'to', 'distinguish', 'yso', \"'s\", 'from', '\\r\\n', 'extra', 'galactic', 'sources', 'more', 'than', 'by', 'the', 'lack', 'of', 'yso', \"'s\", 'at', 'very', 'low', 'luminosities', '\\r\\n', 'a', 'spatial', 'clustering', 'analysis', 'shows', 'that', 'the', 'nominally', 'less', 'evolved', 'yso', \"'s\", 'are', '\\r\\n', 'more', 'highly', 'clustered', 'than', 'the', 'later', 'stages', 'and', 'that', 'the', 'background', '\\r\\n', 'extra', 'galactic', 'population', 'can', 'be', 'fit', 'by', 'the', 'same', 'two', 'point', 'correlation', 'function', '\\r\\n', 'as', 'seen', 'in', 'other', 'extra', 'galactic', 'studies', 'we', 'also', 'present', 'a', 'table', 'of', 'matches', '\\r\\n', 'between', 'several', 'previous', 'infrared', 'and', 'x', 'ray', 'studies', 'of', 'the', 'serpens', 'yso', '\\r\\n', 'population', 'and', 'our', 'spitzer', 'data', 'set', '\\r\\n'], ['  ', 'partial', 'cubes', 'are', 'isometric', 'subgraphs', 'of', 'hypercubes', 'structures', 'on', 'a', 'graph', '\\r\\n', 'defined', 'by', 'means', 'of', 'semicubes', 'and', \"djokovi\\\\'{c\", \"'s\", 'and', 'winkler', \"'s\", 'relations', 'play', '\\r\\n', 'an', 'important', 'role', 'in', 'the', 'theory', 'of', 'partial', 'cubes', 'these', 'structures', 'are', 'employed', '\\r\\n', 'in', 'the', 'paper', 'to', 'characterize', 'bipartite', 'graphs', 'and', 'partial', 'cubes', 'of', 'arbitrary', '\\r\\n', 'dimension', 'new', 'characterizations', 'are', 'established', 'and', 'new', 'proofs', 'of', 'some', 'known', '\\r\\n', 'results', 'are', 'given', '\\r\\n  ', 'the', 'operations', 'of', 'cartesian', 'product', 'and', 'pasting', 'and', 'expansion', 'and', '\\r\\n', 'contraction', 'processes', 'are', 'utilized', 'in', 'the', 'paper', 'to', 'construct', 'new', 'partial', 'cubes', '\\r\\n', 'from', 'old', 'ones', 'in', 'particular', 'the', 'isometric', 'and', 'lattice', 'dimensions', 'of', 'finite', '\\r\\n', 'partial', 'cubes', 'obtained', 'by', 'means', 'of', 'these', 'operations', 'are', 'calculated', '\\r\\n']]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "tokenized_words_list = []\n",
    "\n",
    "\n",
    "for index, row in df_short.iterrows():\n",
    "    text = row['abstract']\n",
    "    tokenized_words = tokenize_and_normalize(text)\n",
    "    tokenized_words_list.append(tokenized_words)\n",
    "\n",
    "\n",
    "print(tokenized_words_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  ', 'a', 'fully', 'differential', 'calculation', 'in', 'perturbative', 'quantum', 'chromodynamics', 'is', '\\r\\n', 'presented', 'for', 'the', 'production', 'of', 'massive', 'photon', 'pairs', 'at', 'hadron', 'colliders', 'all', '\\r\\n', 'next', 'to', 'leading', 'order', 'perturbative', 'contributions', 'from', 'quark', 'antiquark', '\\r\\n', 'gluon-(anti)quark', 'and', 'gluon', 'gluon', 'subprocesses', 'are', 'included', 'as', 'well', 'as', '\\r\\n', 'all', 'orders', 'resummation', 'of', 'initial', 'state', 'gluon', 'radiation', 'valid', 'at', '\\r\\n', 'next', 'to', 'next', 'to', 'leading', 'logarithmic', 'accuracy', 'the', 'region', 'of', 'phase', 'space', 'is', '\\r\\n', 'specified', 'in', 'which', 'the', 'calculation', 'is', 'most', 'reliable', 'good', 'agreement', 'is', '\\r\\n', 'demonstrated', 'with', 'data', 'from', 'the', 'fermilab', 'tevatron', 'and', 'predictions', 'are', 'made', 'for', '\\r\\n', 'more', 'detailed', 'tests', 'with', 'cdf', 'and', 'do', 'data', 'predictions', 'are', 'shown', 'for', '\\r\\n', 'distributions', 'of', 'diphoton', 'pairs', 'produced', 'at', 'the', 'energy', 'of', 'the', 'large', 'hadron', '\\r\\n', 'collider', 'lhc', 'distributions', 'of', 'the', 'diphoton', 'pairs', 'from', 'the', 'decay', 'of', 'a', 'higgs', '\\r\\n', 'boson', 'are', 'contrasted', 'with', 'those', 'produced', 'from', 'qcd', 'processes', 'at', 'the', 'lhc', 'showing', '\\r\\n', 'that', 'enhanced', 'sensitivity', 'to', 'the', 'signal', 'can', 'be', 'obtained', 'with', 'judicious', '\\r\\n', 'selection', 'of', 'events', '\\r\\n']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_words_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "132\n",
      "164\n",
      "42\n",
      "45\n",
      "161\n",
      "172\n",
      "149\n",
      "323\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    a = len(tokenized_words_list[i])\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amos/miniconda3/envs/GNNpapersearch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "data = HeteroData()\n",
    "\n",
    "data['paper'].num_nodes = 300\n",
    "data['author'].num_nodes = 200\n",
    "data['paper', 'written_by', 'author'].edge_index = torch.tensor([\n",
    "    [1,2,2],\n",
    "    [100,102,105]\n",
    "]) \n",
    "\n",
    "1,100\n",
    "2,102\n",
    "data['author'].x = torch.tensor([\n",
    "    [3,4,1],\n",
    "    [1,1,1],\n",
    "    [0,0,0]\n",
    "])\n",
    "\n",
    "\n",
    "data['paper', 'written_by', 'author'].edge_attr = torch.tensor([\n",
    "    [3,4,1],\n",
    "    [1,1,1],\n",
    "    [0,0,0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  paper={ num_nodes=300 },\n",
       "  author={\n",
       "    num_nodes=200,\n",
       "    x=[3, 3],\n",
       "  },\n",
       "  (paper, written_by, author)={\n",
       "    edge_index=[2, 3],\n",
       "    edge_attr=[3, 3],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "\n",
    "transform = T.Compose([\n",
    "       T.RemoveIsolatedNodes(),\n",
    "       T.ToUndirected(merge=False), # don't merge reversed edges into the original edge type\n",
    "       T.RemoveDuplicatedEdges(),\n",
    "])\n",
    "data_before = data\n",
    "data = transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  paper={ num_nodes=2 },\n",
       "  author={\n",
       "    num_nodes=3,\n",
       "    x=[3, 3],\n",
       "  },\n",
       "  (paper, written_by, author)={\n",
       "    edge_index=[2, 3],\n",
       "    edge_attr=[3, 3],\n",
       "  },\n",
       "  (author, rev_written_by, paper)={\n",
       "    edge_index=[2, 3],\n",
       "    edge_attr=[3, 3],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1],\n",
       "        [0, 1, 2]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['paper', 'written_by', 'author'].edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ilgar\\Desktop\\Uni\\Natural Language Processing\\GNNpapersearch\\src\\make_the_graph\\pipeline_example.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ilgar/Desktop/Uni/Natural%20Language%20Processing/GNNpapersearch/src/make_the_graph/pipeline_example.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load English tokenizer, tagger, parser and NER\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ilgar/Desktop/Uni/Natural%20Language%20Processing/GNNpapersearch/src/make_the_graph/pipeline_example.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39men_core_web_sm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ilgar/Desktop/Uni/Natural%20Language%20Processing/GNNpapersearch/src/make_the_graph/pipeline_example.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Process whole documents\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ilgar/Desktop/Uni/Natural%20Language%20Processing/GNNpapersearch/src/make_the_graph/pipeline_example.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m text \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mWhen Sebastian Thrun started working on self-driving cars at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ilgar/Desktop/Uni/Natural%20Language%20Processing/GNNpapersearch/src/make_the_graph/pipeline_example.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mGoogle in 2007, few people outside of the company took him \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ilgar/Desktop/Uni/Natural%20Language%20Processing/GNNpapersearch/src/make_the_graph/pipeline_example.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mseriously. “I can tell you very senior CEOs of major American \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ilgar/Desktop/Uni/Natural%20Language%20Processing/GNNpapersearch/src/make_the_graph/pipeline_example.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcar companies would shake my hand and turn away because I wasn’t \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ilgar/Desktop/Uni/Natural%20Language%20Processing/GNNpapersearch/src/make_the_graph/pipeline_example.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mworth talking to,” said Thrun, in an interview with Recode earlier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ilgar/Desktop/Uni/Natural%20Language%20Processing/GNNpapersearch/src/make_the_graph/pipeline_example.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthis week.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNNpapersearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
