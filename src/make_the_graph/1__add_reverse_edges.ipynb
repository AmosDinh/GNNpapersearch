{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\I549512\\AppData\\Local\\miniconda3\\envs\\gnnpapersearch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "heterodata = pickle.load(open(\"hetero_graph_final.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  paper={\n",
       "    license=[1000],\n",
       "    doi=[1000],\n",
       "    pages=[1000],\n",
       "    journal=[1000],\n",
       "    date=[1000],\n",
       "    id=[1000],\n",
       "    name=[1000],\n",
       "    num_nodes=1000,\n",
       "  },\n",
       "  author={\n",
       "    name=[2886],\n",
       "    num_nodes=2886,\n",
       "  },\n",
       "  category={\n",
       "    name=[108],\n",
       "    num_nodes=108,\n",
       "  },\n",
       "  word={\n",
       "    name=[10667],\n",
       "    num_nodes=10667,\n",
       "  },\n",
       "  journal={\n",
       "    name=[574],\n",
       "    num_nodes=574,\n",
       "  },\n",
       "  (paper, written_by, author)={ edge_index=[2, 3068] },\n",
       "  (paper, has_category, category)={ edge_index=[2, 1502] },\n",
       "  (paper, has_word, word)={\n",
       "    edge_index=[2, 77703],\n",
       "    edge_attr=[77703],\n",
       "  },\n",
       "  (paper, has_titleword, word)={ edge_index=[2, 7462] },\n",
       "  (paper, in_journal, journal)={ edge_index=[2, 1000] },\n",
       "  (word, co_occurs_with, word)={\n",
       "    edge_index=[2, 499132],\n",
       "    edge_attr=[499132],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heterodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "heterodata['paper']['license'] = [license if license is not None else 'None' for license in heterodata['paper']['license']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove isolated nodes\n",
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([\n",
    "       #T.RemoveIsolatedNodes(),\n",
    "       T.RemoveDuplicatedEdges(),\n",
    "       # T.ToUndirected(merge=False) # don't merge reversed edges into the original edge type\n",
    "])\n",
    "heterodata = transform(heterodata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper\n",
      "del paper license\n",
      "del paper doi\n",
      "del paper pages\n",
      "del paper journal\n",
      "del paper date\n",
      "del paper id\n",
      "del paper name\n",
      "del paper num_nodes\n",
      "author\n",
      "del author name\n",
      "del author num_nodes\n",
      "category\n",
      "del category name\n",
      "del category num_nodes\n",
      "word\n",
      "del word name\n",
      "del word num_nodes\n",
      "journal\n",
      "del journal name\n",
      "del journal num_nodes\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# id mapping so we can remove the attributes and then remove isolated nodes, later we can add the attributes back\n",
    "id_dict = {}\n",
    "more_mappings = {}\n",
    "heterodata_dict = heterodata.to_dict()\n",
    "for nodetype in heterodata.node_types:\n",
    "    print(nodetype)\n",
    "    ids = torch.arange(heterodata[nodetype].num_nodes) \n",
    "    id_mapping = {i.item():name for i, name in zip(ids, heterodata[nodetype].name)}\n",
    "    if nodetype =='paper':\n",
    "        for key in ['license','doi','pages','journal','date','id']:\n",
    "            more_mappings[key] = {i.item():name for i, name in zip(ids, heterodata[nodetype][key])}\n",
    "    \n",
    "    heterodata[nodetype].x = ids.squeeze(-1)\n",
    "    id_dict[nodetype] = id_mapping\n",
    "    \n",
    "    for key in heterodata_dict[nodetype].keys():\n",
    "        if key != 'x':\n",
    "            print('del', nodetype, key)\n",
    "            del heterodata[nodetype][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correct edge types:\n",
    "transform = T.Compose([\n",
    "       T.RemoveIsolatedNodes(),\n",
    "       #    T.RemoveDuplicatedEdges(),\n",
    "       # T.ToUndirected(merge=False) # don't merge reversed edges into the original edge type\n",
    "])\n",
    "\n",
    "heterodata = transform(heterodata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  paper={ x=[1000] },\n",
       "  author={ x=[2886] },\n",
       "  category={ x=[108] },\n",
       "  word={ x=[10604] },\n",
       "  journal={ x=[574] },\n",
       "  (paper, written_by, author)={ edge_index=[2, 3067] },\n",
       "  (paper, has_category, category)={ edge_index=[2, 1502] },\n",
       "  (paper, has_word, word)={\n",
       "    edge_index=[2, 57073],\n",
       "    edge_attr=[57073],\n",
       "  },\n",
       "  (paper, has_titleword, word)={ edge_index=[2, 7354] },\n",
       "  (paper, in_journal, journal)={ edge_index=[2, 1000] },\n",
       "  (word, co_occurs_with, word)={\n",
       "    edge_index=[2, 499132],\n",
       "    edge_attr=[499132],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heterodata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "heterodata = T.ToUndirected()(heterodata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper\n",
      "author\n",
      "category\n",
      "word\n",
      "journal\n"
     ]
    }
   ],
   "source": [
    "# map back\n",
    "for nodetype in heterodata.node_types:\n",
    "    print(nodetype)\n",
    "    id_mapping = id_dict[nodetype]\n",
    "    heterodata[nodetype].name = [id_mapping[i.item()] for i in heterodata[nodetype].x]\n",
    "    \n",
    "    heterodata[nodetype].num_nodes = len(heterodata[nodetype].name)\n",
    "    \n",
    "    if nodetype =='paper':\n",
    "        for key in ['license','doi','pages','journal','date','id']:\n",
    "            heterodata[nodetype][key] = [more_mappings[key][i.item()] for i in heterodata[nodetype].x]\n",
    "    \n",
    "    del heterodata[nodetype].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "if not os.path.exists('arxiv_author_paper_graph_no_features_bare.pkl'):\n",
    "    pickle.dump(heterodata, open(\"arxiv_author_paper_graph_no_features_bare.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper nans to median\n",
    "import math\n",
    "non_nan = [num for num in heterodata['paper'].pages if not math.isnan(num)]\n",
    "median = non_nan[len(non_nan)//2]\n",
    "heterodata['paper'].pages = [median if math.isnan(num) else num for num in heterodata['paper'].pages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical \n",
    "unique_licenses= list(set(heterodata['paper'].license))\n",
    "categories = {unique_licenses[i]:i for i in range(len(unique_licenses))}\n",
    "heterodata['paper'].license = [categories[license] for license in heterodata['paper'].license]\n",
    "import torch \n",
    "# onehot\n",
    "onehot = torch.nn.functional.one_hot(torch.tensor(heterodata['paper'].license ))\n",
    "heterodata['paper'].license = onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_timestamp = heterodata['paper'].date[0]\n",
    "import datetime\n",
    "start_timestamp_epoch = datetime.datetime(1970, 1, 1, 0, 0, 0, tzinfo=given_timestamp.tzinfo)\n",
    "\n",
    "# Calculate the difference in hours divided by \n",
    "difference_in_hours_by_4 = (given_timestamp - start_timestamp_epoch).total_seconds() // (3600*4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "heterodata['paper'].date = [((timestamp - start_timestamp_epoch).total_seconds() // (3600*4)) for timestamp in heterodata['paper'].date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "journal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "from tqdm.auto import tqdm\n",
    "# for each node type create the x attribute from the embeddings of the node names\n",
    "for node_type in heterodata.node_types:\n",
    "    print(node_type)\n",
    "    # do it in batches\n",
    "    x = torch.zeros(heterodata[node_type].num_nodes, 384).float()\n",
    "    batch_size = 10000\n",
    "    for i in tqdm(range(0, heterodata[node_type].num_nodes, 10000)):\n",
    "        x[i:i+10000] = torch.tensor(embedder.encode(heterodata[node_type].name[i:i+10000], convert_to_numpy=True))\n",
    "        \n",
    "        \n",
    "    heterodata[node_type].x = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "heterodata['paper'].x = torch.concatenate((heterodata['paper'].x,heterodata['paper'].license), dim=1)\n",
    "heterodata['paper'].x  = torch.concatenate((heterodata['paper'].x, torch.tensor(heterodata['paper'].date).unsqueeze(1)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node_type in heterodata.node_types:\n",
    "    del heterodata[node_type].name \n",
    "    if node_type == 'paper':\n",
    "        for key in ['license','doi','pages','journal','date','id']:\n",
    "            del heterodata[node_type][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "import pickle\n",
    "import os\n",
    "if not os.path.exists('arxiv_author_paper_graph_training_v1.pkl'):\n",
    "    pickle.dump(heterodata, open(\"arxiv_author_paper_graph_training_v1.pkl\", \"wb\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
