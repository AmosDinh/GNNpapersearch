{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "heterodata = pickle.load(open(\"hetero_graph_final.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heterodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heterodata['paper']['license'] = [license if license is not None else 'None' for license in heterodata['paper']['license']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove isolated nodes\n",
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([\n",
    "       #T.RemoveIsolatedNodes(),\n",
    "       T.RemoveDuplicatedEdges(),\n",
    "       # T.ToUndirected(merge=False) # don't merge reversed edges into the original edge type\n",
    "])\n",
    "heterodata = transform(heterodata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# id mapping so we can remove the attributes and then remove isolated nodes, later we can add the attributes back\n",
    "id_dict = {}\n",
    "more_mappings = {}\n",
    "heterodata_dict = heterodata.to_dict()\n",
    "for nodetype in heterodata.node_types:\n",
    "    print(nodetype)\n",
    "    ids = torch.arange(heterodata[nodetype].num_nodes) \n",
    "    id_mapping = {i.item():name for i, name in zip(ids, heterodata[nodetype].name)}\n",
    "    if nodetype =='paper':\n",
    "        for key in ['license','doi','pages','journal','date','id']:\n",
    "            more_mappings[key] = {i.item():name for i, name in zip(ids, heterodata[nodetype][key])}\n",
    "    \n",
    "    heterodata[nodetype].x = ids.squeeze(-1)\n",
    "    id_dict[nodetype] = id_mapping\n",
    "    \n",
    "    for key in heterodata_dict[nodetype].keys():\n",
    "        if key != 'x':\n",
    "            print('del', nodetype, key)\n",
    "            del heterodata[nodetype][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correct edge types:\n",
    "transform = T.Compose([\n",
    "       T.RemoveIsolatedNodes(),\n",
    "       #    T.RemoveDuplicatedEdges(),\n",
    "       # T.ToUndirected(merge=False) # don't merge reversed edges into the original edge type\n",
    "])\n",
    "\n",
    "heterodata = transform(heterodata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heterodata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heterodata = T.ToUndirected()(heterodata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map back\n",
    "for nodetype in heterodata.node_types:\n",
    "    print(nodetype)\n",
    "    id_mapping = id_dict[nodetype]\n",
    "    heterodata[nodetype].name = [id_mapping[i.item()] for i in heterodata[nodetype].x]\n",
    "    \n",
    "    heterodata[nodetype].num_nodes = len(heterodata[nodetype].name)\n",
    "    \n",
    "    if nodetype =='paper':\n",
    "        for key in ['license','doi','pages','journal','date','id']:\n",
    "            heterodata[nodetype][key] = [more_mappings[key][i.item()] for i in heterodata[nodetype].x]\n",
    "    \n",
    "    del heterodata[nodetype].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "if not os.path.exists('arxiv_author_paper_graph_no_features_bare.pkl'):\n",
    "    pickle.dump(heterodata, open(\"arxiv_author_paper_graph_no_features_bare.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper nans to median\n",
    "import math\n",
    "non_nan = [num for num in heterodata['paper'].pages if not math.isnan(num)]\n",
    "median = non_nan[len(non_nan)//2]\n",
    "heterodata['paper'].pages = [median if math.isnan(num) else num for num in heterodata['paper'].pages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical \n",
    "unique_licenses= list(set(heterodata['paper'].license))\n",
    "categories = {unique_licenses[i]:i for i in range(len(unique_licenses))}\n",
    "heterodata['paper'].license = [categories[license] for license in heterodata['paper'].license]\n",
    "import torch \n",
    "# onehot\n",
    "onehot = torch.nn.functional.one_hot(torch.tensor(heterodata['paper'].license ))\n",
    "heterodata['paper'].license = onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_timestamp = heterodata['paper'].date[0]\n",
    "import datetime\n",
    "start_timestamp_epoch = datetime.datetime(1970, 1, 1, 0, 0, 0, tzinfo=given_timestamp.tzinfo)\n",
    "\n",
    "# Calculate the difference in hours divided by \n",
    "difference_in_hours_by_4 = (given_timestamp - start_timestamp_epoch).total_seconds() // (3600*4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heterodata['paper'].date = [((timestamp - start_timestamp_epoch).total_seconds() // (3600*4)) for timestamp in heterodata['paper'].date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "from tqdm.auto import tqdm\n",
    "# for each node type create the x attribute from the embeddings of the node names\n",
    "for node_type in heterodata.node_types:\n",
    "    print(node_type)\n",
    "    # do it in batches\n",
    "    x = torch.zeros(heterodata[node_type].num_nodes, 384).float()\n",
    "    batch_size = 10000\n",
    "    for i in tqdm(range(0, heterodata[node_type].num_nodes, 10000)):\n",
    "        x[i:i+10000] = torch.tensor(embedder.encode(heterodata[node_type].name[i:i+10000], convert_to_numpy=True))\n",
    "        \n",
    "        \n",
    "    heterodata[node_type].x = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heterodata['paper'].x = torch.concatenate((heterodata['paper'].x,heterodata['paper'].license), dim=1)\n",
    "heterodata['paper'].x  = torch.concatenate((heterodata['paper'].x, torch.tensor(heterodata['paper'].date).unsqueeze(1)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node_type in heterodata.node_types:\n",
    "    del heterodata[node_type].name \n",
    "    if node_type == 'paper':\n",
    "        for key in ['license','doi','pages','journal','date','id']:\n",
    "            del heterodata[node_type][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "import pickle\n",
    "import os\n",
    "if not os.path.exists('arxiv_author_paper_graph_training_v1.pkl'):\n",
    "    pickle.dump(heterodata, open(\"arxiv_author_paper_graph_training_v1.pkl\", \"wb\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
