{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Reciprocal Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch==2.1.0  torchvision==0.16.0 torchtext==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "#! pip install pyg_lib torch_scatter torch_sparse torch_cluster -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html # torch_spline_conv\n",
    "! pip install torch_geometric\n",
    "! pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
    "#! pip install torch_sparse -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html\n",
    "#! pip install torch_scatter -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html\n",
    "#! pip install pyg_lib -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html\n",
    "! pip install sentence-transformers\n",
    "! pip install torcheval\n",
    "! pip install matplotlib\n",
    "! pip install pandas\n",
    "! pip install tensorboard\n",
    "! pip install weaviate-client\n",
    "\n",
    "! pip install -U pip setuptools wheel\n",
    "! pip install -U spacy\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of dataset on disk:  2.279761238 gb\n",
      "loading saved heterodata object\n",
      "for skill job edges keep top k edges per job, k is  50\n",
      "keep tensor(1208056) of total 16289586\n"
     ]
    }
   ],
   "source": [
    "from graph_sampler import get_datasets, equal_edgeweight_hgt_sampler, get_minibatch_count, add_reverse_edge_original_attributes_and_label_inplace, get_hgt_linkloader, get_single_minibatch_count, sampler_for_init\n",
    "\n",
    "train_data, val_data, test_data = get_datasets(get_edge_attr=False, filename=ROOT_FOLDER+'HeteroData_Learnings_normalized_triangles_withadditionaldata_v1.pt', filter_top_k=True, top_k=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.TransE import TransE\n",
    "from models.DistMult import DistMult\n",
    "from models.HGT import HGT\n",
    "import torch_geometric\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, gnn : torch.nn.Module, head , node_types, edge_types, ggn_output_dim, pnorm=1):\n",
    "        super().__init__()\n",
    "        # edge_type onehot lookup table with keys\n",
    "        # node_type onehot lookup table with keys\n",
    "        self.node_type_embedding = torch.nn.Embedding(len(node_types), ggn_output_dim) # hidden channels should be the output dim of gnn\n",
    "        \n",
    "        self.edge_types = edge_types\n",
    "        for edge_type in edge_types:\n",
    "            if edge_type[1].startswith('rev_'):\n",
    "                self.edge_types.remove(edge_type)\n",
    "        \n",
    "        # create edge to int mapping\n",
    "        self.edgeindex_lookup = {edge_type:torch.tensor(i)  for i, edge_type in enumerate(edge_types)}\n",
    "            \n",
    "        # hidden channels should be the output dim of gnn\n",
    "        if head=='TransE': \n",
    "            self.head = TransE(len(node_types), len(edge_types) , ggn_output_dim, p_norm= pnorm, margin=0.5)  # KGE head with loss function\n",
    "        elif head=='DistMult':\n",
    "            self.head = DistMult(len(node_types), len(edge_types) , ggn_output_dim, p_norm= pnorm, margin=0.5)  # KGE head with loss function\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        self.gnn = gnn\n",
    "        \n",
    "    \n",
    "\n",
    "    def forward(self, hetero_data1, target_edge_type, edge_label_index, edge_label, hetero_data2=None, get_head_fn='loss'):\n",
    "        \n",
    "        if hetero_data2 is not None:\n",
    "            assert target_edge_type[0] != target_edge_type[2], 'when passing two data objects, the edge type has to contain two different node types'\n",
    "            head_embeddings = self.gnn(hetero_data1.x_dict, hetero_data1.edge_index_dict)[target_edge_type[0]][edge_label_index[0,:]]\n",
    "            tail_embeddings = self.gnn(hetero_data2.x_dict, hetero_data2.edge_index_dict)[target_edge_type[2]][edge_label_index[1,:]]\n",
    "        else:\n",
    "            assert target_edge_type[0] == target_edge_type[2], 'when passing one data object, the edge type has to contain the same node types'\n",
    "\n",
    "\n",
    "            embeddings = self.gnn(hetero_data1.x_dict, hetero_data1.edge_index_dict)\n",
    "            head_embeddings = embeddings[target_edge_type[0]][edge_label_index[0,:]]\n",
    "            tail_embeddings = embeddings[target_edge_type[2]][edge_label_index[1,:]]\n",
    "\n",
    "        \n",
    "        edgeindex = self.edgeindex_lookup[target_edge_type]\n",
    "        if get_head_fn=='loss':\n",
    "            loss = self.head.loss(head_embeddings, edgeindex.to(device), tail_embeddings, edge_label)\n",
    "            return loss\n",
    "        elif get_head_fn=='forward':\n",
    "            return self.head.forward(head_embeddings, edgeindex.to(device), tail_embeddings)\n",
    "    \n",
    "        \n",
    "metadata = train_data.metadata()\n",
    "# add selfloops\n",
    "for node_type in train_data.node_types:\n",
    "    metadata[1].append((node_type, 'self_loop', node_type))    \n",
    "    \n",
    "out_channels = 256\n",
    "hidden_channels = 256\n",
    "num_heads = 8\n",
    "num_layers = 2\n",
    "pnorm = 2\n",
    "head = 'TransE'\n",
    "gnn = HGT(hidden_channels=out_channels, out_channels=out_channels, num_heads=num_heads, num_layers=num_layers, node_types=train_data.node_types, data_metadata=metadata)\n",
    "\n",
    "model = Model(gnn, head=head, node_types=metadata[0], edge_types=metadata[1], ggn_output_dim=out_channels, pnorm=pnorm)\n",
    "#torch_geometric.compile(model, dynamic=True)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "batch_size = 32\n",
    "num_node_types = len(train_data.node_types)\n",
    "print('num_node_types', num_node_types)\n",
    "one_hop_neighbors = (20 * batch_size)//num_node_types # per relationship type\n",
    "two_hop_neighbors = (20 * 8 * batch_size)//num_node_types # per relationship type\n",
    "three_hop_neighbors = (20 * 8 * 3 * batch_size)//num_node_types # per relationship type\n",
    "num_neighbors = [one_hop_neighbors, two_hop_neighbors] # three_hop_neighbors\n",
    "# num_neighbors [36, 363, 1454]\n",
    "\n",
    "print('num_neighbors', num_neighbors)\n",
    "print('avg_num_neighbors', [num_neighbors[0]/batch_size,num_neighbors[1]/batch_size,  num_neighbors[2]/batch_size if len(num_neighbors)==3 else 0 ])\n",
    "\n",
    "sampler_for_init = equal_edgeweight_hgt_sampler(train_data, batch_size, True, 'triplet', 1, num_neighbors, num_workers=0, prefetch_factor=None, pin_memory=True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to init until all node types are present\n",
    "model.eval()\n",
    "for i, (same_nodetype, target_edge_type, minibatch) in tqdm(enumerate(sampler_for_init)):\n",
    "\n",
    "    # batching is different depending on if node types in edge are same or different\n",
    "    print(target_edge_type)\n",
    "    if same_nodetype:\n",
    "        \n",
    "        minibatch, edge_label_index, edge_label, input_edge_ids, global_node_ids = minibatch\n",
    "        #print(minibatch['jobs'].x.device, edge_label_index.device, edge_label.device)\n",
    "        loss = model(minibatch.to(device), target_edge_type, edge_label_index.to(device), edge_label.to(device))\n",
    "        #loss, pos, neg = model(minibatch, target_edge_type, edge_label_index, edge_label)\n",
    "    else:\n",
    "        try:\n",
    "            minibatchpart1, minibatchpart2, edge_label_index, edge_label, input_edge_id, global_src_ids, global_dst_ids = minibatch\n",
    "        except ValueError as err:\n",
    "            print('value error', err)\n",
    "            continue # for skill qual edges sometimes for some reason only 5 instead of 7 elements returned\n",
    "        #print(minibatchpart1['jobs'].device, minibatchpart2['jobs'].device, edge_label_index.device, edge_label.device)\n",
    "        loss = model(minibatchpart1.to(device), target_edge_type, edge_label_index.to(device), edge_label.to(device), minibatchpart2.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get parameters form statedcit\n",
    "model.load_state_dict = torch.load(ROOT_FOLDER+'models/learningall_hgt_20231104_123858_margin05_pnorm2_llr0.0002_bs32_neighbors_106_853_head_TransE_hiddenchannels_256_outchannels_256_numheads_8_numlayers_2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test set\n",
    "# init dimensions of model by training it\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np \n",
    "\n",
    "def evaluate(model, n_negatives, model_folder, on='test'):\n",
    "    num_neighbors = [int(x) for x in model_folder.split('neighbors_')[1].split('head')[0].strip('_').split('_')]\n",
    "    \n",
    "    model.to(device)\n",
    "    mrrs = []\n",
    "   \n",
    "    \n",
    "    #test_sampler = get_hgt_linkloader(test_data, input_edgetype, 1, False, 'triplet', n_negatives, num_neighbors, num_workers=0, prefetch_factor=None, pin_memory=True)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # test data\n",
    "    train_data_text, val_data_text, test_data_text = get_datasets(get_edge_attr=False, filename=ROOT_FOLDER+'HeteroData_Learnings_normalized_triangles_withadditionaldata_v1.pt', filter_top_k=True, top_k=50, remove_text_attr=False)\n",
    "    input_edgetype = ('people', 'rev_course_and_programs_student', 'courses_and_programs')\n",
    "    if on=='test':\n",
    "        add_reverse_edge_original_attributes_and_label_inplace(test_data['courses_and_programs', 'course_and_programs_student', 'people'], reverse_edge=test_data[input_edgetype] )\n",
    "        test_sampler = get_hgt_linkloader(test_data, input_edgetype, 1, False, 'triplet', n_negatives, num_neighbors, num_workers=0, prefetch_factor=None, pin_memory=True)\n",
    "    elif on =='train':\n",
    "        add_reverse_edge_original_attributes_and_label_inplace(train_data['courses_and_programs', 'course_and_programs_student', 'people'], reverse_edge=train_data[input_edgetype] )\n",
    "        test_sampler = get_hgt_linkloader(train_data, input_edgetype, 1, False, 'triplet', n_negatives, num_neighbors, num_workers=0, prefetch_factor=None, pin_memory=True)\n",
    "    # test data\n",
    "    model.eval()\n",
    "    mrr_per_edge_type = {}\n",
    "    rank_per_edge_type = {}\n",
    "    best_mrr, best_differences, best_src_nodes, best_dst_nodes = 0, None, None, None\n",
    "    for i, (same_nodetype, target_edge_type, minibatch) in tqdm(enumerate(sampler_for_init)):\n",
    "        if i==1000:\n",
    "            break\n",
    "        \n",
    "        print(target_edge_type)\n",
    "        if same_nodetype:\n",
    "            minibatch, edge_label_index, edge_label, input_edge_ids, global_node_ids = minibatch\n",
    "            #print(minibatch['jobs'].x.device, edge_label_index.device, edge_label.device)\n",
    "            differences = model(minibatch.to(device), target_edge_type, edge_label_index.to(device), edge_label.to(device))\n",
    "            #loss, pos, neg = model(minibatch, target_edge_type, edge_label_index, edge_label)\n",
    "        else:\n",
    "            try:\n",
    "                minibatchpart1, minibatchpart2, edge_label_index, edge_label, input_edge_id, global_src_ids, global_dst_ids = minibatch\n",
    "            except ValueError as err:\n",
    "                print('value error', err)\n",
    "                continue # for skill qual edges sometimes for some reason only 5 instead of 7 elements returned\n",
    "            #print(minibatchpart1['jobs'].device, minibatchpart2['jobs'].device, edge_label_index.device, edge_label.device)\n",
    "            differences = model(minibatchpart1.to(device), target_edge_type, edge_label_index.to(device), edge_label.to(device), minibatchpart2.to(device))\n",
    "        #loss.backward()\n",
    "\n",
    "        # define mrr with differences and labels\n",
    "        # get rank of positive edge from tensor, positive edge is first in batch\n",
    "\n",
    "        differences = -1* differences.cpu().detach().numpy()\n",
    "        edge_label = edge_label.cpu().detach().numpy()\n",
    "        rank = (differences < differences[0]).sum()\n",
    "\n",
    "        # reciprocal\n",
    "        mrr = 1/(rank+1)\n",
    "        if mrr > best_mrr:\n",
    "            best_mrr = mrr\n",
    "            best_differences = differences\n",
    "            best_src_nodes = global_src_nodes\n",
    "            best_dst_nodes = global_dst_nodes\n",
    "            print('new best mrr', best_mrr)\n",
    "            \n",
    "    \n",
    "        mrrs.append(mrr)\n",
    "        mrr_per_edge_type[target_edge_type] = mrr_per_edge_type.get(target_edge_type, []) + [mrr]\n",
    "        rank_per_edge_type[target_edge_type] = rank_per_edge_type.get(target_edge_type, []) + [rank]\n",
    "        print('mrr',target_edge_type,mrr)\n",
    "        print(rank)\n",
    "        \n",
    "    print('mean mrr',np.mean(mrrs))\n",
    "    print('mean rank',1/np.mean(mrrs))\n",
    "    # mean rank\n",
    "    model.to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32494])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((test_data['courses_and_programs', 'course_and_programs_student', 'people'].edge_index[0,:].unique(),test_data['courses_and_programs', 'course_and_programs_student', 'people'].edge_label_index[0,:].unique())).unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set(test_data['courses_and_programs', 'course_and_programs_student', 'people'].edge_label_index[0,:].unique().tolist())\n",
    "a2 = set(train_data['courses_and_programs', 'course_and_programs_student', 'people'].edge_label_index[0,:].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = set(torch.cat((train_data['courses_and_programs', 'course_and_programs_student', 'people'].edge_index[0,:].unique(),train_data['courses_and_programs', 'course_and_programs_student', 'people'].edge_label_index[0,:].unique())).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9335"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.intersection(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of dataset on disk:  2.279761238 gb\n",
      "loading saved heterodata object\n",
      "for skill job edges keep top k edges per job, k is  50\n",
      "keep tensor(1208056) of total 16289586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:06,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best mrr 0.0024752475247524753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:12,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best mrr 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:14,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best mrr 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [45:27,  2.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean mrr 0.17139854320945072\n",
      "mean rank 5.834355305914065\n"
     ]
    }
   ],
   "source": [
    "# evaluate(model_3layereuclid, 10000, model_folder3layereuclid, on='test')\n",
    "# evaluate(model_2layereuclid, 10000, model_folder2layereuclid, on='test')\n",
    "evaluate(model_2layerp1, 10000, model_folder2layerp1, on='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of dataset on disk:  2.279761238 gb\n",
      "loading saved heterodata object\n",
      "for skill job edges keep top k edges per job, k is  50\n",
      "keep tensor(1208056) of total 16289586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:15, 15.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best mrr 8.13206473123526e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:23, 10.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best mrr 0.000500751126690035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:36,  8.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best mrr 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [01:00,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best mrr 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [23:41,  7.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean mrr 0.13983406037281088\n",
      "mean rank 7.151333497245987\n",
      "size of dataset on disk:  2.279761238 gb\n",
      "loading saved heterodata object\n",
      "for skill job edges keep top k edges per job, k is  50\n",
      "keep tensor(1208056) of total 16289586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:09,  9.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best mrr 0.0001484560570071259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:13,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best mrr 0.005494505494505495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:18,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best mrr 0.16666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:27,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best mrr 0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [01:25,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best mrr 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [09:34,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean mrr 0.08684513895969696\n",
      "mean rank 11.514749264942502\n",
      "size of dataset on disk:  2.279761238 gb\n",
      "loading saved heterodata object\n",
      "for skill job edges keep top k edges per job, k is  50\n",
      "keep tensor(1208056) of total 16289586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:05,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best mrr 0.00016452780519907864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:07,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best mrr 0.02127659574468085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:12,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best mrr 0.1111111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:20,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best mrr 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [10:11,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean mrr 0.11901925778350261\n",
      "mean rank 8.402001647657823\n"
     ]
    }
   ],
   "source": [
    "evaluate(model_3layereuclid, 10000, model_folder3layereuclid, on='train')\n",
    "evaluate(model_2layereuclid, 10000, model_folder2layereuclid, on='train')\n",
    "evaluate(model_2layerp1, 10000, model_folder2layerp1, on='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(modele4, 50000, model_foldere4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(modele4euclidean, 50000, model_foldere4euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(modele5, 50000, model_foldere5, on='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(modele4, 50000, model_foldere4, on='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(modele4euclidean, 30000, model_foldere4euclidean, on='train')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
