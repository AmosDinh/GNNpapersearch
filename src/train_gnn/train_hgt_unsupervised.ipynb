{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e3c4b139-0996-4471-990d-08f1a7bbe79a",
          "showTitle": false,
          "title": ""
        },
        "id": "F4RRC1_n6aMc"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "70ff1fe6-6bf8-4150-ac65-9a9f45fe8df1",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aHsO__GO6aMf",
        "outputId": "c0f44d69-bd70-4992-de8e-79c60d2eb103"
      },
      "outputs": [],
      "source": [
        "! pip install torch==2.1.0  torchvision==0.16.0 torchtext==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "#! pip install pyg_lib torch_scatter torch_sparse torch_cluster -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html # torch_spline_conv\n",
        "! pip install torch_geometric\n",
        "! pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
        "#! pip install torch_sparse -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html\n",
        "#! pip install torch_scatter -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html\n",
        "#! pip install pyg_lib -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html\n",
        "! pip install sentence-transformers\n",
        "! pip install torcheval\n",
        "! pip install matplotlib\n",
        "! pip install pandas\n",
        "! pip install tensorboard\n",
        "! pip install weaviate-client\n",
        "\n",
        "! pip install -U pip setuptools wheel\n",
        "! pip install -U spacy\n",
        "! python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "efe06c00-84ff-49e4-89a9-cad18f63b110",
          "showTitle": false,
          "title": ""
        },
        "id": "uFRl8z0Y6aMh",
        "outputId": "aac7d0d2-0957-4854-db8b-75721b3c6d15"
      },
      "outputs": [],
      "source": [
        "from graph_sampler import get_datasets, equal_edgeweight_hgt_sampler, get_minibatch_count, add_reverse_edge_original_attributes_and_label_inplace, get_hgt_linkloader, get_single_minibatch_count, sampler_for_init\n",
        "\n",
        "train_data, val_data, test_data = get_datasets(get_edge_attr=False, filter_top_k=True, top_k=15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch_geometric.nn.kge import TransE\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "from torch_geometric.nn.kge import KGEModel\n",
        "\n",
        "# adapted and taken from https://github.com/pyg-team/pytorch_geometric/blob/master/torch_geometric/nn/kge/transe.py\n",
        "\n",
        "class TransE(KGEModel):\n",
        "    r\"\"\"The TransE model from the `\"Translating Embeddings for Modeling\n",
        "    Multi-Relational Data\" <https://proceedings.neurips.cc/paper/2013/file/\n",
        "    1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf>`_ paper.\n",
        "\n",
        "    :class:`TransE` models relations as a translation from head to tail\n",
        "    entities such that\n",
        "\n",
        "    .. math::\n",
        "        \\mathbf{e}_h + \\mathbf{e}_r \\approx \\mathbf{e}_t,\n",
        "\n",
        "    resulting in the scoring function:\n",
        "\n",
        "    .. math::\n",
        "        d(h, r, t) = - {\\| \\mathbf{e}_h + \\mathbf{e}_r - \\mathbf{e}_t \\|}_p\n",
        "\n",
        "    .. note::\n",
        "\n",
        "        For an example of using the :class:`TransE` model, see\n",
        "        `examples/kge_fb15k_237.py\n",
        "        <https://github.com/pyg-team/pytorch_geometric/blob/master/examples/\n",
        "        kge_fb15k_237.py>`_.\n",
        "\n",
        "    Args:\n",
        "        num_nodes (int): The number of nodes/entities in the graph.\n",
        "        num_relations (int): The number of relations in the graph.\n",
        "        hidden_channels (int): The hidden embedding size.\n",
        "        margin (int, optional): The margin of the ranking loss.\n",
        "            (default: :obj:`1.0`)\n",
        "        p_norm (int, optional): The order embedding and distance normalization.\n",
        "            (default: :obj:`1.0`)\n",
        "        sparse (bool, optional): If set to :obj:`True`, gradients w.r.t. to the\n",
        "            embedding matrices will be sparse. (default: :obj:`False`)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_nodes: int,\n",
        "        num_relations: int,\n",
        "        hidden_channels: int,\n",
        "        margin: float = 1.0,\n",
        "        p_norm: float = 1.0,\n",
        "        sparse: bool = False,\n",
        "    ):\n",
        "        super().__init__(num_nodes, num_relations, hidden_channels, sparse)\n",
        "\n",
        "        self.p_norm = p_norm\n",
        "        self.margin = margin\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        bound = 6. / math.sqrt(self.hidden_channels)\n",
        "        torch.nn.init.uniform_(self.node_emb.weight, -bound, bound)\n",
        "        torch.nn.init.uniform_(self.rel_emb.weight, -bound, bound)\n",
        "        F.normalize(self.rel_emb.weight.data, p=self.p_norm, dim=-1,\n",
        "                    out=self.rel_emb.weight.data)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        head_embeddings: Tensor,\n",
        "        rel_type,\n",
        "        tail_embeddings: Tensor,\n",
        "    ) -> Tensor:\n",
        "        #head = self.node_emb(head_index)\n",
        "        rel = self.rel_emb(rel_type)  # Amos: only learn the relation embeddings, others are learned with GNN\n",
        "        #tail = self.node_emb(tail_index)\n",
        "\n",
        "        head = F.normalize(head_embeddings, p=self.p_norm, dim=-1)\n",
        "        tail = F.normalize(tail_embeddings, p=self.p_norm, dim=-1)\n",
        "        # Calculate *negative* TransE norm:\n",
        "        negative_norm = -((head + rel) - tail).norm(p=self.p_norm, dim=-1)\n",
        "        return negative_norm\n",
        "\n",
        "    \n",
        "    def get_embedding(self,\n",
        "                      embedding,\n",
        "                      rel_type,\n",
        "                        have_head_or_tail\n",
        "                      ):\n",
        "        rel = self.rel_emb(rel_type)\n",
        "        embedding = F.normalize(embedding, p=self.p_norm, dim=-1)\n",
        "        if have_head_or_tail == 'head':\n",
        "            return embedding + rel\n",
        "        else:\n",
        "            return embedding - rel\n",
        "    \n",
        "    \n",
        "    def loss(\n",
        "        self,\n",
        "        head_embeddings: Tensor,\n",
        "        rel_type: Tensor,\n",
        "        tail_embeddings: Tensor,\n",
        "        labels: Tensor, # labels 0 or 1\n",
        "    ) -> Tensor:\n",
        "        pos_mask = labels == 1\n",
        "        neg_mask = labels == 0\n",
        "        \n",
        "        pos_score = self(head_embeddings[pos_mask], rel_type, tail_embeddings[pos_mask])\n",
        "        neg_score = self(head_embeddings[neg_mask], rel_type, tail_embeddings[neg_mask])\n",
        "        loss = F.margin_ranking_loss(\n",
        "            pos_score,\n",
        "            neg_score,\n",
        "            target=torch.ones_like(pos_score), # 1 for similarity, -1 for dissimilarity\n",
        "            margin=self.margin,\n",
        "        )\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch_geometric.nn import HGTConv, Linear\n",
        "import torch \n",
        "\n",
        "class HGT(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels, num_heads, num_layers, node_types, data_metadata):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_dict = torch.nn.ModuleDict()\n",
        "        for node_type in node_types:\n",
        "            self.lin_dict[node_type] = Linear(-1, hidden_channels)\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            conv = HGTConv(hidden_channels, hidden_channels, data_metadata,\n",
        "                           num_heads, group='sum')\n",
        "            self.convs.append(conv)\n",
        "\n",
        "        self.lin = Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        x_dict = {\n",
        "            node_type: self.lin_dict[node_type](x).relu_()\n",
        "            for node_type, x in x_dict.items()\n",
        "        }\n",
        "\n",
        "        for conv in self.convs:\n",
        "            x_dict = conv(x_dict, edge_index_dict)\n",
        "\n",
        "        return x_dict\n",
        "    \n",
        "# if __name__ == '__main__':\n",
        "    # model = HGT(hidden_channels=64, out_channels=4, num_heads=2, num_layers=1, node_types=data.node_types, data_metadata=data.metadata())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0d1eaa98-6d39-486c-a20c-67b7bec2fdda",
          "showTitle": false,
          "title": ""
        },
        "id": "XlDW7VD16aMi",
        "outputId": "db3f7d9a-0f09-43cf-97e2-95de1b3d0ece"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# from models.TransE import TransE\n",
        "# from models.DistMult import DistMult\n",
        "# from models.HGT import HGT\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, gnn : torch.nn.Module, head :  torch.nn.Module, node_types, edge_types, ggn_output_dim, pnorm=1):\n",
        "        super().__init__()\n",
        "        # edge_type onehot lookup table with keys\n",
        "        # node_type onehot lookup table with keys\n",
        "        self.node_type_embedding = torch.nn.Embedding(len(node_types), ggn_output_dim) # hidden channels should be the output dim of gnn\n",
        "\n",
        "        self.edge_types = edge_types\n",
        "        for edge_type in edge_types:\n",
        "            if edge_type[1].startswith('rev_'):\n",
        "                self.edge_types.remove(edge_type)\n",
        "\n",
        "        # create edge to int mapping\n",
        "        self.edgeindex_lookup = {edge_type:torch.tensor(i)  for i, edge_type in enumerate(edge_types)}\n",
        "\n",
        "        # hidden channels should be the output dim of gnn\n",
        "        if head=='TransE':\n",
        "            self.head = TransE(len(node_types), len(edge_types) , ggn_output_dim, p_norm= pnorm, margin=0.5)  # KGE head with loss function\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.gnn = gnn\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, hetero_data1, target_edge_type, edge_label_index, edge_label, hetero_data2=None, get_head_fn='loss'):\n",
        "\n",
        "        if hetero_data2 is not None:\n",
        "            assert target_edge_type[0] != target_edge_type[2], 'when passing two data objects, the edge type has to contain two different node types'\n",
        "            head_embeddings = self.gnn(hetero_data1.x_dict, hetero_data1.edge_index_dict)[target_edge_type[0]][edge_label_index[0,:]]\n",
        "            tail_embeddings = self.gnn(hetero_data2.x_dict, hetero_data2.edge_index_dict)[target_edge_type[2]][edge_label_index[1,:]]\n",
        "        else:\n",
        "            assert target_edge_type[0] == target_edge_type[2], 'when passing one data object, the edge type has to contain the same node types'\n",
        "\n",
        "\n",
        "            embeddings = self.gnn(hetero_data1.x_dict, hetero_data1.edge_index_dict)\n",
        "            head_embeddings = embeddings[target_edge_type[0]][edge_label_index[0,:]]\n",
        "            tail_embeddings = embeddings[target_edge_type[2]][edge_label_index[1,:]]\n",
        "        \n",
        "\n",
        "        edgeindex = self.edgeindex_lookup[target_edge_type]\n",
        "        if get_head_fn=='loss':\n",
        "            loss = self.head.loss(head_embeddings, edgeindex.to(device), tail_embeddings, edge_label)\n",
        "            return loss\n",
        "        elif get_head_fn=='forward':\n",
        "            return self.head.forward(head_embeddings, edgeindex.to(device), tail_embeddings)\n",
        "\n",
        "\n",
        "metadata = train_data.metadata()\n",
        "# add selfloops\n",
        "for node_type in train_data.node_types:\n",
        "    metadata[1].append((node_type, 'self_loop', node_type))\n",
        "\n",
        "out_channels = 256\n",
        "hidden_channels = 256\n",
        "num_heads = 8\n",
        "num_layers = 3\n",
        "pnorm = 2\n",
        "head = 'TransE'\n",
        "gnn = HGT(hidden_channels=out_channels, out_channels=out_channels, num_heads=num_heads, num_layers=num_layers, node_types=train_data.node_types, data_metadata=metadata)\n",
        "\n",
        "model = Model(gnn, head=head, node_types=metadata[0], edge_types=metadata[1], ggn_output_dim=out_channels, pnorm=pnorm)\n",
        "#torch_geometric.compile(model, dynamic=True)\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4ebad4a8-bb51-45f9-8f93-8036fd9e89ae",
          "showTitle": false,
          "title": ""
        },
        "id": "uG7RfwhM6aMj"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "import os \n",
        "batch_size = 32\n",
        "num_node_types = len(train_data.node_types)\n",
        "print('num_node_types', num_node_types)\n",
        "one_hop_neighbors = (20 * batch_size)//num_node_types # per relationship type\n",
        "two_hop_neighbors = (20 * 8 * batch_size)//num_node_types # per relationship type\n",
        "three_hop_neighbors = (20 * 8 * 3 * batch_size)//num_node_types # per relationship type\n",
        "num_neighbors = [one_hop_neighbors, two_hop_neighbors, three_hop_neighbors] # three_hop_neighbors\n",
        "# num_neighbors [36, 363, 1454]\n",
        "\n",
        "print('num_neighbors', num_neighbors)\n",
        "print('avg_num_neighbors', [num_neighbors[0]/batch_size,num_neighbors[1]/batch_size,  num_neighbors[2]/batch_size if len(num_neighbors)==3 else 0 ])\n",
        "\n",
        "train_sampler = equal_edgeweight_hgt_sampler(train_data, batch_size, True, 'triplet', 1, num_neighbors, num_workers=0, prefetch_factor=None, pin_memory=True)\n",
        "val_sampler = equal_edgeweight_hgt_sampler(val_data, batch_size, True, 'triplet', 1, num_neighbors, num_workers=0, prefetch_factor=None, pin_memory=True)\n",
        "\n",
        "\n",
        "learning_rate = 2e-4\n",
        "# torch get optimizer by string name\n",
        "optimizer = 'Adam'\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #2e-15\n",
        "\n",
        "\n",
        "# create a tensorboard writer\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "neighbors = '_'.join([str(n) for n in num_neighbors])\n",
        "\n",
        "from pathlib import Path\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "ROOT_FOLDER = 'cache/'\n",
        "summary_folder = Path(ROOT_FOLDER+f'runs/all_hgt_{timestamp}_mg05_norm{pnorm}_lr{learning_rate}_bs{batch_size}_neigh_{neighbors}_h_{head}_hid_{hidden_channels}_out_{out_channels}_numh_{num_heads}_numl_{num_layers}'.replace('.', '_'))\n",
        "print('summary_folder', summary_folder)\n",
        "writer = SummaryWriter(summary_folder)\n",
        "print('writer',summary_folder)\n",
        "# make dir \n",
        "\n",
        "model.train()\n",
        "start_epoch = 1\n",
        "total_minibatches = get_minibatch_count(train_data, batch_size)\n",
        "for epoch in range(start_epoch, start_epoch+1):\n",
        "    for i, (same_nodetype, target_edge_type, minibatch) in tqdm(enumerate(train_sampler), total=total_minibatches):\n",
        "\n",
        "        try:\n",
        "            if i%3==0:\n",
        "                optimizer.zero_grad()\n",
        "            # batching is different depending on if node types in edge are same or different\n",
        "            print(target_edge_type)\n",
        "            if same_nodetype:\n",
        "\n",
        "                minibatch, edge_label_index, edge_label, input_edge_ids, global_node_ids = minibatch\n",
        "                #print(minibatch['jobs'].x.device, edge_label_index.device, edge_label.device)\n",
        "                loss = model(minibatch.to(device), target_edge_type, edge_label_index.to(device), edge_label.to(device))\n",
        "                #loss, pos, neg = model(minibatch, target_edge_type, edge_label_index, edge_label)\n",
        "            else:\n",
        "                try:\n",
        "                    minibatchpart1, minibatchpart2, edge_label_index, edge_label, input_edge_id, global_src_ids, global_dst_ids = minibatch\n",
        "                except ValueError as err:\n",
        "                    print('value error', err)\n",
        "                    continue # for skill qual edges sometimes for some reason only 5 instead of 7 elements returned\n",
        "                #print(minibatchpart1['jobs'].device, minibatchpart2['jobs'].device, edge_label_index.device, edge_label.device)\n",
        "                loss = model(minibatchpart1.to(device), target_edge_type, edge_label_index.to(device), edge_label.to(device), minibatchpart2.to(device))\n",
        "\n",
        "\n",
        "            loss.backward()\n",
        "            if i%3==2:\n",
        "                optimizer.step()\n",
        "\n",
        "            total_samples_seen = i * batch_size\n",
        "            writer.add_scalar('Loss/train', loss.item(), total_samples_seen)\n",
        "\n",
        "            if i == total_minibatches-1:\n",
        "                print(f'{i} loss: {loss.item():.4f}')\n",
        "                writer.add_scalar('Epoch Loss/train', loss.item(), total_samples_seen)\n",
        "\n",
        "            # print loss and minibatch in the same line\n",
        "            print(f'{i} loss: {loss.item():.4f}', end='\\r')\n",
        "\n",
        "            if i % 300 == 0 or i == total_minibatches-1:\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    val_loss = 0\n",
        "                    for _ in range(3):\n",
        "                        try:\n",
        "                            same_nodetype, target_edge_type, minibatch = next(val_sampler)\n",
        "                        except StopIteration:\n",
        "                            val_sampler = iter(val_sampler)\n",
        "                            same_nodetype, target_edge_type, minibatch = next(val_sampler)\n",
        "\n",
        "                        if same_nodetype:\n",
        "                            minibatch, edge_label_index, edge_label, input_edge_ids, global_node_ids = minibatch\n",
        "                            #print(minibatch['jobs'].x.device, edge_label_index.device, edge_label.device)\n",
        "                            val_loss += model(minibatch.to(device), target_edge_type, edge_label_index.to(device), edge_label.to(device))\n",
        "                            #loss, pos, neg = model(minibatch, target_edge_type, edge_label_index, edge_label)\n",
        "                        else:\n",
        "                            try:\n",
        "                                minibatchpart1, minibatchpart2, edge_label_index, edge_label, input_edge_id, global_src_ids, global_dst_ids = minibatch\n",
        "                            except ValueError:\n",
        "                                continue\n",
        "\n",
        "                            #print(minibatchpart1['jobs'].device, minibatchpart2['jobs'].device, edge_label_index.device, edge_label.device)\n",
        "                            val_loss += model(minibatchpart1.to(device), target_edge_type, edge_label_index.to(device), edge_label.to(device), minibatchpart2.to(device))\n",
        "\n",
        "                val_loss /= 3\n",
        "                if i == 0:\n",
        "                    writer.add_scalar('Epoch Loss/val', val_loss, total_samples_seen)\n",
        "                    writer.add_scalar('Loss/val', val_loss, total_samples_seen)\n",
        "                elif i == total_minibatches-1:\n",
        "                    writer.add_scalar('Epoch Loss/val', val_loss, total_samples_seen)\n",
        "                else:\n",
        "                    writer.add_scalar('Loss/val', val_loss, total_samples_seen)\n",
        "\n",
        "\n",
        "                print(f'val_loss: {val_loss:.4f}', end='\\r')\n",
        "                model.train()\n",
        "\n",
        "            writer.flush()\n",
        "\n",
        "            if i % 10000 == 0 or i == total_minibatches-1:\n",
        "                folder = 'models'\n",
        "                if not os.path.exists(folder):\n",
        "                    os.makedirs(folder)\n",
        "\n",
        "                \n",
        "                run_folder = Path(summary_folder)\n",
        "                if not os.path.exists(run_folder):\n",
        "                    os.makedirs(run_folder)\n",
        "\n",
        "                print('saving model to', run_folder)\n",
        "                # save model and optimizer\n",
        "                is_epoch = f'Ep{epoch}_' if i == total_minibatches-1 else ''\n",
        "                torch.save({\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    }, run_folder/f'{is_epoch}model_samplesseen{total_samples_seen}.pt')\n",
        "\n",
        "        except IndexError:\n",
        "            print('indexerror')\n",
        "            pass\n",
        "\n",
        "writer.close()"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "learnings_training_v1_learning_unsupervised",
      "widgets": {}
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
