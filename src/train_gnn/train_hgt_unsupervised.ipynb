{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e3c4b139-0996-4471-990d-08f1a7bbe79a",
          "showTitle": false,
          "title": ""
        },
        "id": "F4RRC1_n6aMc"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "70ff1fe6-6bf8-4150-ac65-9a9f45fe8df1",
          "showTitle": false,
          "title": ""
        },
        "id": "aHsO__GO6aMf",
        "outputId": "c0f44d69-bd70-4992-de8e-79c60d2eb103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision==0.16.0 in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: torchtext==0.16.0 in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: torchaudio==2.1.0 in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0) (9.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (4.66.1)\n",
            "Requirement already satisfied: torchdata==0.7.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (0.7.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.0->torchtext==0.16.0) (2.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.4.0\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/pyg_lib-0.3.1%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_scatter-2.1.2%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_sparse-0.6.18%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_cluster-1.6.3%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_spline_conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_spline_conv-1.2.2%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (932 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m932.1/932.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.23.5)\n",
            "Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n",
            "Successfully installed pyg_lib-0.3.1+pt21cu121 torch_cluster-1.6.3+pt21cu121 torch_scatter-2.1.2+pt21cu121 torch_sparse-0.6.18+pt21cu121 torch_spline_conv-1.2.2+pt21cu121\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=4fdce78aa74a177bf250fb4ecc394357eac00e6337f640b1130a180389ac513c\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.99\n",
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.5.0)\n",
            "Installing collected packages: torcheval\n",
            "Successfully installed torcheval-0.0.7\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.14.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.59.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.5.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
            "Collecting weaviate-client\n",
            "  Downloading weaviate_client-3.25.3-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.3/120.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.30.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (2.31.0)\n",
            "Collecting validators<1.0.0,>=0.21.2 (from weaviate-client)\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting authlib<2.0.0,>=1.2.1 (from weaviate-client)\n",
            "  Downloading Authlib-1.2.1-py2.py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=3.2 in /usr/local/lib/python3.10/dist-packages (from authlib<2.0.0,>=1.2.1->weaviate-client) (41.0.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (2023.11.17)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.2->authlib<2.0.0,>=1.2.1->weaviate-client) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.2->authlib<2.0.0,>=1.2.1->weaviate-client) (2.21)\n",
            "Installing collected packages: validators, authlib, weaviate-client\n",
            "Successfully installed authlib-1.2.1 validators-0.22.0 weaviate-client-3.25.3\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.7.2)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-69.0.2-py3-none-any.whl (819 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.5/819.5 kB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
            "Installing collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-23.3.1 setuptools-69.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (69.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Downloading spacy-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cloudpathlib, weasel, spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.6.1\n",
            "    Uninstalling spacy-3.6.1:\n",
            "      Successfully uninstalled spacy-3.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.7.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cloudpathlib-0.16.0 spacy-3.7.2 weasel-0.3.4\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m2023-12-09 14:30:03.678295: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-09 14:30:03.678362: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-09 14:30:03.678393: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-09 14:30:03.687201: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-09 14:30:04.841165: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 3.6.0\n",
            "    Uninstalling en-core-web-sm-3.6.0:\n",
            "      Successfully uninstalled en-core-web-sm-3.6.0\n",
            "Successfully installed en-core-web-sm-3.7.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "! pip install torch==2.1.0  torchvision==0.16.0 torchtext==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "#! pip install pyg_lib torch_scatter torch_sparse torch_cluster -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html # torch_spline_conv\n",
        "! pip install torch_geometric\n",
        "! pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
        "#! pip install torch_sparse -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html\n",
        "#! pip install torch_scatter -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html\n",
        "#! pip install pyg_lib -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html\n",
        "! pip install sentence-transformers\n",
        "! pip install torcheval\n",
        "! pip install matplotlib\n",
        "! pip install pandas\n",
        "! pip install tensorboard\n",
        "! pip install weaviate-client\n",
        "\n",
        "! pip install -U pip setuptools wheel\n",
        "! pip install -U spacy\n",
        "! python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Running in Google Colab\")\n",
        "else:\n",
        "    print(\"Not running in Google Colab\")\n"
      ],
      "metadata": {
        "id": "lRNhqgh_7DAm",
        "outputId": "a757498f-2326-48ec-ed1c-8ddde7a88e1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if IN_COLAB:\n",
        "  import os\n",
        "  import numpy as np\n",
        "  from pathlib import Path\n",
        "  from tqdm.auto import tqdm\n",
        "  from google.colab import drive\n",
        "  import matplotlib.pyplot as plt\n",
        "  drive.mount('/content/drive',force_remount=True)\n",
        "  DRIVE_FOLDER = Path('/content/drive/MyDrive/Data/')\n",
        "  DRIVE_GNN_PROGESS_FOLDER = DRIVE_FOLDER/'GNNPaper_Search'\n",
        "  ROOT_FOLDER = DRIVE_GNN_PROGESS_FOLDER"
      ],
      "metadata": {
        "id": "RP5x0JNO61vX",
        "outputId": "c17341d6-651b-49d4-ec05-a14dbc427bd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DEZPdnS36aMh"
      },
      "outputs": [],
      "source": [
        "ROOT_FOLDER ='src/'\n",
        "import shutil\n",
        "\n",
        "if IN_COLAB and not 'unzipped' in globals():\n",
        "  #!unzip /content/drive/MyDrive/Data/HeteroData_Learnings_normalized_triangles_withadditionaldata_v1.pt\n",
        "  source_path = \"/content/drive/MyDrive/Data/HeteroData_Learnings_normalized_triangles_withadditionaldata_v1.pt\"\n",
        "  destination_path = \"/content/dataset.pt\"\n",
        "\n",
        "  #unzipped = True\n",
        "  shutil.copy(source_path, destination_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import HGTConv, Linear\n",
        "import torch\n",
        "\n",
        "class HGT(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels, num_heads, num_layers, node_types, data_metadata):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_dict = torch.nn.ModuleDict()\n",
        "        for node_type in node_types:\n",
        "            self.lin_dict[node_type] = Linear(-1, hidden_channels)\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            conv = HGTConv(hidden_channels, hidden_channels, data_metadata,\n",
        "                           num_heads, group='sum')\n",
        "            self.convs.append(conv)\n",
        "\n",
        "        self.lin = Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        x_dict = {\n",
        "            node_type: self.lin_dict[node_type](x).relu_()\n",
        "            for node_type, x in x_dict.items()\n",
        "        }\n",
        "\n",
        "        for conv in self.convs:\n",
        "            x_dict = conv(x_dict, edge_index_dict)\n",
        "\n",
        "        return x_dict\n",
        ""
      ],
      "metadata": {
        "id": "DuGxaBLyAkIu",
        "outputId": "bc78ea5b-2452-485e-b4d6-8d8e022ef1fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-c8528d2599a6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHGTConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHGT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msbm_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomPartitionGraphDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmixhop_synthetic_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMixHopSyntheticDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexplainer_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplainerDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minfection_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInfectionDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mba2motif_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBA2MotifDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/datasets/explainer_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotif_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMotifGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplanation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/explain/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplainerConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mThresholdConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexplanation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplanation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHeteroExplanation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexplainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/explain/algorithm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplainerAlgorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdummy_explainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDummyExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgnn_explainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGNNExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcaptum_explainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCaptumExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpg_explainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPGExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/explain/algorithm/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mModelReturnType\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMessagePassing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEdgeType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNodeType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mk_hop_subgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_parallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataParallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mto_hetero_transformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_hetero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mto_hetero_with_bases_transformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_hetero_with_bases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mto_fixed_size_transformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_fixed_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPositionalEncoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTemporalEncoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/to_hetero_with_bases_transformer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMessagePassing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/conv/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcugraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msage_conv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCuGraphSAGEConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgraph_conv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgravnet_conv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGravNetConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgated_graph_conv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGatedGraphConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mres_gated_graph_conv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResGatedGraphConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/conv/gravnet_conv.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_cluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mknn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_cluster/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda_spec\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcpu_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         raise ImportError(f\"Could not find module '{library}_cpu' in \"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;31m# static (global) initialization code in order to register custom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m             \u001b[0;31m# operators with the JIT.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_libraries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: libcudart.so.12: cannot open shared object file: No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn.kge import TransE\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "from torch_geometric.nn.kge import KGEModel\n",
        "\n",
        "# adapted and taken from https://github.com/pyg-team/pytorch_geometric/blob/master/torch_geometric/nn/kge/transe.py\n",
        "\n",
        "class TransE(KGEModel):\n",
        "    r\"\"\"The TransE model from the `\"Translating Embeddings for Modeling\n",
        "    Multi-Relational Data\" <https://proceedings.neurips.cc/paper/2013/file/\n",
        "    1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf>`_ paper.\n",
        "\n",
        "    :class:`TransE` models relations as a translation from head to tail\n",
        "    entities such that\n",
        "\n",
        "    .. math::\n",
        "        \\mathbf{e}_h + \\mathbf{e}_r \\approx \\mathbf{e}_t,\n",
        "\n",
        "    resulting in the scoring function:\n",
        "\n",
        "    .. math::\n",
        "        d(h, r, t) = - {\\| \\mathbf{e}_h + \\mathbf{e}_r - \\mathbf{e}_t \\|}_p\n",
        "\n",
        "    .. note::\n",
        "\n",
        "        For an example of using the :class:`TransE` model, see\n",
        "        `examples/kge_fb15k_237.py\n",
        "        <https://github.com/pyg-team/pytorch_geometric/blob/master/examples/\n",
        "        kge_fb15k_237.py>`_.\n",
        "\n",
        "    Args:\n",
        "        num_nodes (int): The number of nodes/entities in the graph.\n",
        "        num_relations (int): The number of relations in the graph.\n",
        "        hidden_channels (int): The hidden embedding size.\n",
        "        margin (int, optional): The margin of the ranking loss.\n",
        "            (default: :obj:`1.0`)\n",
        "        p_norm (int, optional): The order embedding and distance normalization.\n",
        "            (default: :obj:`1.0`)\n",
        "        sparse (bool, optional): If set to :obj:`True`, gradients w.r.t. to the\n",
        "            embedding matrices will be sparse. (default: :obj:`False`)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_nodes: int,\n",
        "        num_relations: int,\n",
        "        hidden_channels: int,\n",
        "        margin: float = 1.0,\n",
        "        p_norm: float = 1.0,\n",
        "        sparse: bool = False,\n",
        "    ):\n",
        "        super().__init__(num_nodes, num_relations, hidden_channels, sparse)\n",
        "\n",
        "        self.p_norm = p_norm\n",
        "        self.margin = margin\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        bound = 6. / math.sqrt(self.hidden_channels)\n",
        "        torch.nn.init.uniform_(self.node_emb.weight, -bound, bound)\n",
        "        torch.nn.init.uniform_(self.rel_emb.weight, -bound, bound)\n",
        "        F.normalize(self.rel_emb.weight.data, p=self.p_norm, dim=-1,\n",
        "                    out=self.rel_emb.weight.data)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        head_embeddings: Tensor,\n",
        "        rel_type,\n",
        "        tail_embeddings: Tensor,\n",
        "    ) -> Tensor:\n",
        "        #head = self.node_emb(head_index)\n",
        "        rel = self.rel_emb(rel_type)  # Amos: only learn the relation embeddings, others are learned with GNN\n",
        "        #tail = self.node_emb(tail_index)\n",
        "\n",
        "        head = F.normalize(head_embeddings, p=self.p_norm, dim=-1)\n",
        "        tail = F.normalize(tail_embeddings, p=self.p_norm, dim=-1)\n",
        "\n",
        "        # Calculate *negative* TransE norm:\n",
        "        return -((head + rel) - tail).norm(p=self.p_norm, dim=-1)\n",
        "\n",
        "\n",
        "    def get_embedding(self,\n",
        "                      embedding,\n",
        "                      rel_type,\n",
        "                        have_head_or_tail\n",
        "                      ):\n",
        "        rel = self.rel_emb(rel_type)\n",
        "        embedding = F.normalize(embedding, p=self.p_norm, dim=-1)\n",
        "        if have_head_or_tail == 'head':\n",
        "            return embedding + rel\n",
        "        else:\n",
        "            return embedding - rel\n",
        "\n",
        "\n",
        "    def loss(\n",
        "        self,\n",
        "        head_embeddings: Tensor,\n",
        "        rel_type: Tensor,\n",
        "        tail_embeddings: Tensor,\n",
        "        labels: Tensor, # labels 0 or 1\n",
        "    ) -> Tensor:\n",
        "        pos_mask = labels == 1\n",
        "        neg_mask = labels == 0\n",
        "\n",
        "        pos_score = self(head_embeddings[pos_mask], rel_type, tail_embeddings[pos_mask])\n",
        "        neg_score = self(head_embeddings[neg_mask], rel_type, tail_embeddings[neg_mask])\n",
        "\n",
        "        return F.margin_ranking_loss(\n",
        "            pos_score,\n",
        "            neg_score,\n",
        "            target=torch.ones_like(pos_score), # 1 for similarity, -1 for dissimilarity\n",
        "            margin=self.margin,\n",
        "        )"
      ],
      "metadata": {
        "id": "WRsfqCz6AhKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GraphSampler\n",
        "# COMMAND ----------\n",
        "# Sampling\n",
        "# 1. Sample using HGT Sampler as outlined in the paper, using pyg implementations\n",
        "# 2. The sampling is adapted to link prediction, by first sampling random supervision edges of which the nodes create the supervision nodes\n",
        "# a. Dataset is divided across multiple dimensions:\n",
        "#   a.1. Split into Train, Val, Test split (96, 2, 2)\n",
        "#   a.2. Training only: Edges are split into those which are used solely for message passing and those solely used for supervision (80, 20).\n",
        "#        Because an expressive model (HGT) is used, this prevents the model from memorizing supervision edges by their appearance as message passing edges\n",
        "#   a.3. This means Training consists of 96%*80% Message passing Edges, 96%*20% supervision edges, Val contains 2% Supervision Edges, Test contains 2% supervison Edges\n",
        "#   a.4. Validation and Test edges use the Training Message passing Edges as well.\n",
        "# b. For mini-batch sampling in the training phase, first x random edges are sampled as supervision edges.\n",
        "#    For the nodes of these supervision edges, we apply batch-wise HGT Sampling. Due to implementation limitations, for each supervision entity type, the hgt sampling is separate.\n",
        "#    This limitation does not apply for sampled neighbor entity types\n",
        "# during sampling, also the reverse edge of the supervision edge is removed to avoid data leakage\n",
        "\n",
        "\n",
        "# HGT Sampler (See Paper for further reference)\n",
        "# The probablity of a neighbor node s to be sampled depends on the normalized degree of all its edge types connecting it to all source nodes\n",
        "# If neighbor node s is connected to a and b by edge type r, and a has 2 neighbors through edge type r and b has 1 neighbor (node s) through edge type r,\n",
        "# then the sampling probablity of s is (1/2+1)**2 / 2**2, if it were connected through other edge types to the nodes as well, those degrees would be added to the numerator and denominator.\n",
        "# Nodes are sampled without replacement.\n",
        "# This sampling strategy creates more dense mini-batches, because neighbor nodes which are connected to multiple source nodes and by multiple relationship types are sampled more frequently.\n",
        "# Therefore, training is sped up since less node representations have to be computed. Furthermore, as stated in the paper, the sampling method allows to sample a\n",
        "# similar number of neighbors for each edge type, because high-count edge types and low-count edge types are weighted equally. For each neighbor node type T, a fixed number n of nodes is sampled.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch_geometric.data import HeteroData\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "def add_reverse_edge_original_attributes_and_label_inplace(original_edge, reverse_edge):\n",
        "    # add edge label and index and edge attr to reverse edge\n",
        "    if 'edge_attr' in original_edge:\n",
        "        reverse_edge['edge_attr'] = original_edge['edge_attr']\n",
        "    reverse_edge['edge_label'] = original_edge['edge_label']\n",
        "    reverse_edge['edge_label_index'] = original_edge['edge_label_index'].index_select(0, torch.LongTensor([1, 0]))\n",
        "    for key in original_edge.keys():\n",
        "        if key not in ['edge_index', 'edge_attr', 'edge_label', 'edge_label_index']:\n",
        "            reverse_edge[key] = original_edge[key]\n",
        "\n",
        "    return reverse_edge\n",
        "\n",
        "def get_datasets(get_edge_attr=False, filename=None, filter_top_k=False, top_k=50, remove_text_attr=True):\n",
        "    if filename is None:\n",
        "        filename = 'HeteroData_Learnings_normalized_triangles_withadditionaldata_v1.pt'\n",
        "    size = os.path.getsize(filename)\n",
        "    print('size of dataset on disk: ', size/1e9, 'gb')\n",
        "\n",
        "    if os.path.exists(filename):\n",
        "        data = HeteroData.from_dict(torch.load(filename))\n",
        "        print('loading saved heterodata object')\n",
        "\n",
        "\n",
        "\n",
        "    def top_k_mask(scores, indices, top_k ):\n",
        "        # Make sure we are using the GPU\n",
        "        scores = scores.cuda()\n",
        "        indices = indices.cuda()\n",
        "\n",
        "        # Create an empty mask with the same shape as scores\n",
        "        mask = torch.zeros_like(scores, dtype=torch.bool)\n",
        "        # Get the unique indices and their counts\n",
        "        unique_indices, counts = torch.unique(indices, return_counts=True)\n",
        "\n",
        "        # Indices where count > top_k\n",
        "        large_indices = unique_indices[counts > top_k]\n",
        "\n",
        "        # Set mask for indices where count <= top_k\n",
        "        mask[~torch.isin(indices,large_indices)] = True\n",
        "        # For indices where count > 50, we only keep top 50 scores\n",
        "        for idx in tqdm(large_indices):\n",
        "            idx_mask = (indices == idx)\n",
        "            values, idxs = scores[idx_mask].topk(top_k)\n",
        "            a = mask[idx_mask]\n",
        "            a[idxs] = True\n",
        "            mask[idx_mask] = a\n",
        "\n",
        "        return mask.cpu()\n",
        "\n",
        "\n",
        "\n",
        "    if filter_top_k:\n",
        "        print('for skill job edges keep top k edges per job, k is ',top_k)\n",
        "        e = ('skills', 'job_skill', 'jobs')\n",
        "        rev_e = (e[2],'rev_'+e[1],e[0])\n",
        "        cache_dir = 'cache'\n",
        "        if not os.path.exists(cache_dir):\n",
        "            os.makedirs(cache_dir)\n",
        "\n",
        "        mask_path = os.path.join(cache_dir, f'mask{top_k}.pt')\n",
        "\n",
        "        if os.path.isfile(mask_path):\n",
        "            mask = torch.load(mask_path)\n",
        "        else:\n",
        "            mask = top_k_mask(data[e].edge_attr.squeeze(1), data[e].edge_index[1,:], top_k)\n",
        "            torch.save(mask, mask_path)\n",
        "\n",
        "        data[e].edge_attr = data[e].edge_attr[mask]\n",
        "        data[rev_e].edge_attr = data[rev_e].edge_attr[mask]\n",
        "        data[e].edge_index = data[e].edge_index[:,mask]\n",
        "        data[rev_e].edge_index = data[rev_e].edge_index[:,mask]\n",
        "        print('keep',torch.sum(mask), 'of total',mask.shape[0])\n",
        "\n",
        "\n",
        "    from torch_geometric import seed_everything\n",
        "    import torch_geometric.transforms as T\n",
        "    from torch_geometric.utils import sort_edge_index\n",
        "\n",
        "\n",
        "\n",
        "    edge_types = []\n",
        "    rev_edge_types = []\n",
        "    for edge_type in data.edge_types:\n",
        "        if edge_type[1].startswith('rev_'):\n",
        "            rev_edge_types.append(edge_type)\n",
        "        else:\n",
        "            edge_types.append(edge_type)\n",
        "\n",
        "    transform = T.RandomLinkSplit(\n",
        "        is_undirected=True,\n",
        "        edge_types=edge_types,\n",
        "        rev_edge_types=rev_edge_types,\n",
        "        num_val=0.02,\n",
        "        num_test=0.05,\n",
        "        add_negative_train_samples=False, # only adds neg samples for val and test, neg train are added by LinkNeighborLoader. This means for each train batch, negs. are different, for val and train they stay the same\n",
        "        neg_sampling_ratio=1.0,\n",
        "        disjoint_train_ratio=0.3, #  training edges are shared for message passing and supervision\n",
        "        )\n",
        "\n",
        "    seed_everything(14)\n",
        "    # sort by col to speed up sampling later (we can sepcify is_sorted=True in link neighbor loader)\n",
        "    # we actually dont use the sort, because it seems to mess up things, but have not checked if everything works without sorting, so we leave it here\n",
        "    def sort_edges(data):\n",
        "        for edge_type in data.edge_types:\n",
        "            if 'edge_attr' in data[edge_type].keys():\n",
        "                data[edge_type].edge_index, data[edge_type].edge_attr = sort_edge_index(data[edge_type].edge_index, data[edge_type].edge_attr, sort_by_row=False)\n",
        "            else:\n",
        "                data[edge_type].edge_index = sort_edge_index(data[edge_type].edge_index, sort_by_row=False)\n",
        "        return data\n",
        "\n",
        "\n",
        "    def preprocess(data):\n",
        "        if not get_edge_attr:\n",
        "            # delete edge_attr of every edge type\n",
        "            for edge_type in data.edge_types:\n",
        "                del data[edge_type].edge_attr\n",
        "\n",
        "        # delete all keys for every node type except 'x' (e.g. description and title)\n",
        "        for node_type in data.node_types:\n",
        "            keys = list(data[node_type].keys())\n",
        "            for key in keys:\n",
        "                if key != 'x':\n",
        "                    del data[node_type][key]\n",
        "        return data\n",
        "\n",
        "\n",
        "    # change all types to float32 and normalize the triangle columns\n",
        "    for node_type in data.node_types:\n",
        "        for i in range(data[node_type].x.shape[1]):\n",
        "            if data[node_type].x[:,i].max()>5:\n",
        "                #normalize\n",
        "                print('normalizing column ', i, ' of node type ', node_type)\n",
        "                data[node_type].x[:,i] = data[node_type].x[:,i]/data[node_type].x[:,i].max()\n",
        "\n",
        "        data[node_type].x = data[node_type].x.to(torch.float32)\n",
        "\n",
        "\n",
        "\n",
        "    train_data, val_data, test_data = transform(data)\n",
        "    #train_data = sort_edges(train_data)\n",
        "    #val_data = sort_edges(val_data)\n",
        "    #test_data = sort_edges(test_data)\n",
        "    if remove_text_attr:\n",
        "        train_data = preprocess(train_data)\n",
        "        val_data = preprocess(val_data)\n",
        "        test_data = preprocess(test_data)\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from torch_geometric.loader import LinkNeighborLoader\n",
        "from torch_geometric.loader import HGTLoader\n",
        "from torch_geometric.sampler import NegativeSampling\n",
        "\n",
        "def get_hgt_linkloader(data, target_edge, batch_size, is_training, sampling_mode, neg_ratio, num_neighbors_hgtloader, num_workers, prefetch_factor, pin_memory):\n",
        "    # first sample some edges in linkNeighborLoader\n",
        "    # use the nodes of the sampled edges to sample from hgt loader\n",
        "\n",
        "\n",
        "    num_neighbors_linkloader = [0]\n",
        "    #for edge_type in data.edge_types:\n",
        "    #    num_neighbors_linkloader[edge_type] = [0,0]\n",
        "\n",
        "    negative_sampling = NegativeSampling(\n",
        "        mode=sampling_mode, # binary or triplet\n",
        "        amount=neg_ratio  # ratio, like Graphsage # 10\n",
        "        #weight=  # \"Probabilities\" of nodes to be sampled: Node degree follows power law distribution\n",
        "        )\n",
        "\n",
        "    if sampling_mode == 'triplet':\n",
        "        data[target_edge].edge_label = None\n",
        "\n",
        "\n",
        "    linkNeighborLoader = LinkNeighborLoader(\n",
        "            data,\n",
        "            num_neighbors=num_neighbors_linkloader,\n",
        "            edge_label_index=(target_edge, data[target_edge].edge_label_index), # if (edge, None), None means all edges are considered\n",
        "\n",
        "            neg_sampling=negative_sampling, # adds negative samples\n",
        "            batch_size=batch_size,\n",
        "            shuffle=is_training, #is_training\n",
        "            subgraph_type='directional', # contains only sampled edges\n",
        "            #drop_last=True,\n",
        "            num_workers=num_workers,\n",
        "            #disjoint=True # sampled seed node creates its own, disjoint from the rest, subgraph, will add \"batch vector\" to loader output\n",
        "\n",
        "            #num_workers=2,\n",
        "            #prefetch_factor=2\n",
        "            is_sorted = False,\n",
        "            pin_memory=pin_memory,\n",
        "            prefetch_factor=prefetch_factor,\n",
        "    )\n",
        "\n",
        "\n",
        "    def get_hgt(data, input_nodetype, input_mask):\n",
        "        return next(iter(HGTLoader(\n",
        "                data,\n",
        "                # Sample 512 nodes per type and per iteration for 4 iterations\n",
        "                num_samples=num_neighbors_hgtloader,\n",
        "                batch_size=input_mask.shape[0],\n",
        "                input_nodes=(input_nodetype, input_mask),\n",
        "                num_workers=num_workers,\n",
        "                pin_memory=pin_memory,\n",
        "                prefetch_factor=prefetch_factor,\n",
        "            )))\n",
        "\n",
        "\n",
        "    def add_self_loops(data):\n",
        "        for node_type in data.node_types:\n",
        "            data[node_type, 'self_loop', node_type].edge_index = torch.arange(data[node_type].num_nodes).repeat(2,1)\n",
        "        return data\n",
        "\n",
        "\n",
        "    def get_hgt_with_selfloops(loader):\n",
        "\n",
        "\n",
        "        for batch in loader:\n",
        "            if sampling_mode=='triplet':\n",
        "                # original edge_label_index from the whole data object\n",
        "                unmapped_batchids = torch.cat((batch[target_edge[0]].src_index,batch[target_edge[2]].dst_pos_index, batch[target_edge[2]].dst_neg_index.flatten()))\n",
        "                original_node_ids = batch[target_edge[0]].n_id[unmapped_batchids]\n",
        "                original_edge_label_nodes = torch.LongTensor(original_node_ids.unique())\n",
        "\n",
        "                #remapping or sorting is not needed, since nodes are sorted, also in the htg batch, the edges will be the same\n",
        "                src = batch[target_edge[0]].src_index.unsqueeze(0)\n",
        "                src_total = src\n",
        "                for i in range(neg_ratio):\n",
        "                    src_total = torch.cat((src_total,src), dim=1)\n",
        "                dst = torch.cat((batch[target_edge[2]].dst_pos_index, batch[target_edge[2]].dst_neg_index.flatten()),dim=0).unsqueeze(0)\n",
        "\n",
        "                local_edge_label_index = torch.cat((src_total, dst),dim=0)\n",
        "                edge_label = torch.cat((torch.ones(batch[target_edge[2]].dst_pos_index.shape[0]), torch.zeros(batch[target_edge[2]].dst_neg_index.flatten().shape[0])))\n",
        "\n",
        "            elif sampling_mode=='binary':\n",
        "                unmapped_batchids = batch[target_edge].edge_label_index.flatten()\n",
        "                original_node_ids = batch[target_edge[0]].n_id[unmapped_batchids]\n",
        "                original_edge_label_nodes = torch.LongTensor(original_node_ids.unique())\n",
        "\n",
        "            else:\n",
        "                raise Exception('binary or triplet sampling mode')\n",
        "\n",
        "\n",
        "            hgt_batch = get_hgt(data, target_edge[0], original_edge_label_nodes) # 0,1,3,4,5,6,7,8,9,\n",
        "\n",
        "            if sampling_mode=='triplet':\n",
        "\n",
        "                # return message passing edges, and supervision edges/labels, ignore labels/label_indices in the message passing edges\n",
        "                yield add_self_loops(hgt_batch), local_edge_label_index, edge_label, batch[target_edge].input_id, original_node_ids\n",
        "            else: # sampling_mode=='binary':\n",
        "                # return message passing edges, and supervision edges/labels, ignore labels/label_indices in the message passing edges, as well as original edge indices\n",
        "                yield add_self_loops(hgt_batch), batch[target_edge].edge_label_index, batch[target_edge].edge_label, batch[target_edge].input_id, original_node_ids\n",
        "\n",
        "    def get_hgt_2types_with_selfloops(loader):\n",
        "        for batch in loader:\n",
        "            if sampling_mode=='triplet':\n",
        "                original_src_nodes = batch[target_edge[0]].n_id[batch[target_edge[0]].src_index]\n",
        "                original_edge_label_nodes_class1 = torch.LongTensor(original_src_nodes.unique())\n",
        "\n",
        "                original_dst_nodes = batch[target_edge[2]].n_id[torch.cat((batch[target_edge[2]].dst_pos_index, batch[target_edge[2]].dst_neg_index.flatten()))]\n",
        "                original_edge_label_nodes_class2 = torch.LongTensor(original_dst_nodes.unique())\n",
        "\n",
        "\n",
        "                src = batch[target_edge[0]].src_index.unsqueeze(0)\n",
        "                src_total = src\n",
        "                for i in range(neg_ratio):\n",
        "                    src_total = torch.cat((src_total,src), dim=1)\n",
        "\n",
        "                dst = torch.cat((batch[target_edge[2]].dst_pos_index, batch[target_edge[2]].dst_neg_index.flatten()),dim=0).unsqueeze(0)\n",
        "\n",
        "                local_edge_label_index = torch.cat((src_total, dst),dim=0)\n",
        "                edge_label = torch.cat((torch.ones(batch[target_edge[2]].dst_pos_index.shape[0]), torch.zeros(batch[target_edge[2]].dst_neg_index.flatten().shape[0])))\n",
        "\n",
        "            elif sampling_mode=='binary':\n",
        "                original_src_nodes = batch[target_edge[0]].n_id[batch[target_edge].edge_label_index[0,:]]\n",
        "                original_edge_label_nodes_class1 = original_src_nodes.unique()\n",
        "                original_dst_nodes = batch[target_edge[2]].n_id[batch[target_edge].edge_label_index[1,:]]\n",
        "                original_edge_label_nodes_class2 = original_dst_nodes.unique()\n",
        "\n",
        "            else:\n",
        "                raise Exception('binary or triplet sampling mode')\n",
        "\n",
        "            # batch the start and end supervision nodes separately\n",
        "            hgt_batch1 = get_hgt(data, target_edge[0], original_edge_label_nodes_class1)\n",
        "            hgt_batch2 = get_hgt(data, target_edge[2], original_edge_label_nodes_class2)\n",
        "\n",
        "\n",
        "            # ** We dont need to remove any edges ** since the supervision edges wont be sampled by hgt\n",
        "            if sampling_mode=='triplet':\n",
        "                yield add_self_loops(hgt_batch1), add_self_loops(hgt_batch2), local_edge_label_index, edge_label, batch[target_edge].input_id, original_src_nodes, original_dst_nodes\n",
        "            else: # sampling_mode=='binary':\n",
        "                # we can access the corresponding nodes of edge_label_index[0,:] in hgt_batch1[target_edge[0]], those of [1,:] in hgt_batch2...\n",
        "                yield add_self_loops(hgt_batch1), add_self_loops(hgt_batch2), batch[target_edge].edge_label_index, batch[target_edge].edge_label, batch[target_edge].input_id, original_src_nodes, original_dst_nodes\n",
        "\n",
        "\n",
        "    if target_edge[0] == target_edge[2]:\n",
        "        # same edge type, only need to sample once\n",
        "        return get_hgt_with_selfloops(linkNeighborLoader)\n",
        "    else:\n",
        "        return get_hgt_2types_with_selfloops(linkNeighborLoader)\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "def get_minibatch_count(data, batch_size):\n",
        "    batches = []\n",
        "    for edge_type in data.edge_types:\n",
        "        if edge_type[1].startswith('rev_'):\n",
        "            continue\n",
        "        batches.extend([edge_type for _ in range((data[edge_type].edge_label_index.shape[1]+batch_size)//batch_size)])\n",
        "\n",
        "    return len(batches)\n",
        "\n",
        "def get_single_minibatch_count(data, batch_size, edge_type):\n",
        "    return (data[edge_type].edge_label_index.shape[1]+batch_size)//batch_size\n",
        "\n",
        "def uniform_hgt_sampler(data, batch_size, is_training, sampling_mode, neg_sampling_ratio, num_neighbors, num_workers, prefetch_factor, pin_memory):\n",
        "\n",
        "    # return batches from all edgetypes with each \"edge\" being drawn uniformly at random (but we translate the probabilities to batches), last batches of each edge type may be smaller than batch_size\n",
        "    batches = []\n",
        "    loaders = {}\n",
        "    # only the non-reverse edge types for now\n",
        "    for edge_type in data.edge_types:\n",
        "        if edge_type[1].startswith('rev_'):\n",
        "            continue\n",
        "        batches.extend([edge_type for _ in range((data[edge_type].edge_label_index.shape[1]+batch_size)//batch_size)])\n",
        "        loaders[edge_type]=get_hgt_linkloader(data, edge_type, batch_size, is_training, sampling_mode, neg_sampling_ratio, num_neighbors, num_workers, prefetch_factor, pin_memory)\n",
        "\n",
        "    random.seed(14)\n",
        "    random.shuffle(batches)\n",
        "    # set a random random seed again (may affect creating the loaders later for a second epoch)\n",
        "    random.seed()\n",
        "\n",
        "    print('total batches:', len(batches))\n",
        "\n",
        "    for target_edge_type in batches:\n",
        "        if target_edge_type[0] == target_edge_type[2]:\n",
        "            same_nodetype = True\n",
        "        else:\n",
        "            same_nodetype = False\n",
        "        try:\n",
        "            minibatch = next(loaders[target_edge_type])\n",
        "        except StopIteration: # \"reinit\" iterator\n",
        "            loaders[target_edge_type] = get_hgt_linkloader(data, target_edge_type, batch_size, is_training, sampling_mode, neg_sampling_ratio, num_neighbors, num_workers, prefetch_factor, pin_memory)#iter(loaders[target_edge_type])\n",
        "            minibatch = next(loaders[target_edge_type])\n",
        "            pass\n",
        "\n",
        "        yield same_nodetype, target_edge_type, minibatch\n",
        "\n",
        "def sampler_for_init(data, batch_size, is_training, sampling_mode, neg_sampling_ratio, num_neighbors, num_workers, prefetch_factor, pin_memory):\n",
        "    batchcount=[]\n",
        "    batches=[]\n",
        "    loaders = {}\n",
        "    for edge_type in data.edge_types:\n",
        "        if edge_type[1].startswith('rev_'):\n",
        "            continue\n",
        "        batchcount.extend([edge_type for _ in range((data[edge_type].edge_label_index.shape[1]+batch_size)//batch_size)])\n",
        "        batches.append(edge_type)\n",
        "        loaders[edge_type]=get_hgt_linkloader(data, edge_type, batch_size, is_training, sampling_mode, neg_sampling_ratio, num_neighbors, num_workers, prefetch_factor, pin_memory)\n",
        "\n",
        "    print('total batches:', len(batchcount))\n",
        "\n",
        "\n",
        "    # set a random random seed again (may affect creating the loaders later for a second epoch)\n",
        "    random.seed()\n",
        "\n",
        "    for target_edge_type in batches:\n",
        "        if target_edge_type[0] == target_edge_type[2]:\n",
        "            same_nodetype = True\n",
        "        else:\n",
        "            same_nodetype = False\n",
        "        try:\n",
        "            minibatch = next(loaders[target_edge_type])\n",
        "        except StopIteration: # \"reinit\" iterator\n",
        "            loaders[target_edge_type] = get_hgt_linkloader(data, target_edge_type, batch_size, is_training, sampling_mode, neg_sampling_ratio, num_neighbors, num_workers, prefetch_factor, pin_memory)#iter(loaders[target_edge_type])\n",
        "            minibatch = next(loaders[target_edge_type])\n",
        "            pass\n",
        "\n",
        "        yield same_nodetype, target_edge_type, minibatch\n",
        "\n",
        "def equal_edgeweight_hgt_sampler(data, batch_size, is_training, sampling_mode, neg_sampling_ratio, num_neighbors, num_workers, prefetch_factor, pin_memory):\n",
        "    batchcount=[]\n",
        "    batches=[]\n",
        "    loaders = {}\n",
        "    for edge_type in data.edge_types:\n",
        "        if edge_type[1].startswith('rev_'):\n",
        "            continue\n",
        "        batchcount.extend([edge_type for _ in range((data[edge_type].edge_label_index.shape[1]+batch_size)//batch_size)])\n",
        "        batches.append(edge_type)\n",
        "        loaders[edge_type]=get_hgt_linkloader(data, edge_type, batch_size, is_training, sampling_mode, neg_sampling_ratio, num_neighbors, num_workers, prefetch_factor, pin_memory)\n",
        "\n",
        "    print('total batches:', len(batchcount))\n",
        "\n",
        "    batches = random.choices(batches, k=len(batchcount))\n",
        "    # set a random random seed again (may affect creating the loaders later for a second epoch)\n",
        "    random.seed()\n",
        "\n",
        "\n",
        "\n",
        "    for target_edge_type in batches:\n",
        "        if target_edge_type[0] == target_edge_type[2]:\n",
        "            same_nodetype = True\n",
        "        else:\n",
        "            same_nodetype = False\n",
        "        try:\n",
        "            minibatch = next(loaders[target_edge_type])\n",
        "        except StopIteration: # \"reinit\" iterator\n",
        "            loaders[target_edge_type] = get_hgt_linkloader(data, target_edge_type, batch_size, is_training, sampling_mode, neg_sampling_ratio, num_neighbors, num_workers, prefetch_factor, pin_memory)#iter(loaders[target_edge_type])\n",
        "            minibatch = next(loaders[target_edge_type])\n",
        "            pass\n",
        "\n",
        "        yield same_nodetype, target_edge_type, minibatch"
      ],
      "metadata": {
        "id": "6CnTbsIjAcYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "efe06c00-84ff-49e4-89a9-cad18f63b110",
          "showTitle": false,
          "title": ""
        },
        "id": "uFRl8z0Y6aMh",
        "outputId": "aac7d0d2-0957-4854-db8b-75721b3c6d15"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/amos/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size of dataset on disk:  2.279761238 gb\n",
            "loading saved heterodata object\n",
            "for skill job edges keep top k edges per job, k is  50\n",
            "keep tensor(1208056) of total 16289586\n"
          ]
        }
      ],
      "source": [
        "#from graph_sampler import get_datasets, equal_edgeweight_hgt_sampler, get_minibatch_count, add_reverse_edge_original_attributes_and_label_inplace, get_hgt_linkloader, get_single_minibatch_count, sampler_for_init\n",
        "\n",
        "train_data, val_data, test_data = get_datasets(get_edge_attr=False, filename=destination_path, filter_top_k=True, top_k=50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0d1eaa98-6d39-486c-a20c-67b7bec2fdda",
          "showTitle": false,
          "title": ""
        },
        "id": "XlDW7VD16aMi",
        "outputId": "db3f7d9a-0f09-43cf-97e2-95de1b3d0ece"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Model(\n",
              "  (node_type_embedding): Embedding(6, 256)\n",
              "  (head): TransE(6, num_relations=22, hidden_channels=256)\n",
              "  (gnn): HGT(\n",
              "    (lin_dict): ModuleDict(\n",
              "      (courses_and_programs): Linear(-1, 256, bias=True)\n",
              "      (qualifications): Linear(-1, 256, bias=True)\n",
              "      (skills): Linear(-1, 256, bias=True)\n",
              "      (people): Linear(-1, 256, bias=True)\n",
              "      (jobs): Linear(-1, 256, bias=True)\n",
              "      (organizations): Linear(-1, 256, bias=True)\n",
              "    )\n",
              "    (convs): ModuleList(\n",
              "      (0-1): 2 x HGTConv(-1, 256, heads=8)\n",
              "    )\n",
              "    (lin): Linear(256, 256, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from models.TransE import TransE\n",
        "from models.DistMult import DistMult\n",
        "from models.HGT import HGT\n",
        "import torch_geometric\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, gnn : torch.nn.Module, head :  torch.nn.Module, node_types, edge_types, ggn_output_dim, pnorm=1):\n",
        "        super().__init__()\n",
        "        # edge_type onehot lookup table with keys\n",
        "        # node_type onehot lookup table with keys\n",
        "        self.node_type_embedding = torch.nn.Embedding(len(node_types), ggn_output_dim) # hidden channels should be the output dim of gnn\n",
        "\n",
        "        self.edge_types = edge_types\n",
        "        for edge_type in edge_types:\n",
        "            if edge_type[1].startswith('rev_'):\n",
        "                self.edge_types.remove(edge_type)\n",
        "\n",
        "        # create edge to int mapping\n",
        "        self.edgeindex_lookup = {edge_type:torch.tensor(i)  for i, edge_type in enumerate(edge_types)}\n",
        "\n",
        "        # hidden channels should be the output dim of gnn\n",
        "        if head=='TransE':\n",
        "            self.head = TransE(len(node_types), len(edge_types) , ggn_output_dim, p_norm= pnorm, margin=0.5)  # KGE head with loss function\n",
        "        elif head=='DistMult':\n",
        "            self.head = DistMult(len(node_types), len(edge_types) , ggn_output_dim, p_norm= pnorm, margin=0.5)  # KGE head with loss function\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.gnn = gnn\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, hetero_data1, target_edge_type, edge_label_index, edge_label, hetero_data2=None, get_head_fn='loss'):\n",
        "\n",
        "        if hetero_data2 is not None:\n",
        "            assert target_edge_type[0] != target_edge_type[2], 'when passing two data objects, the edge type has to contain two different node types'\n",
        "            head_embeddings = self.gnn(hetero_data1.x_dict, hetero_data1.edge_index_dict)[target_edge_type[0]][edge_label_index[0,:]]\n",
        "            tail_embeddings = self.gnn(hetero_data2.x_dict, hetero_data2.edge_index_dict)[target_edge_type[2]][edge_label_index[1,:]]\n",
        "        else:\n",
        "            assert target_edge_type[0] == target_edge_type[2], 'when passing one data object, the edge type has to contain the same node types'\n",
        "\n",
        "\n",
        "            embeddings = self.gnn(hetero_data1.x_dict, hetero_data1.edge_index_dict)\n",
        "            head_embeddings = embeddings[target_edge_type[0]][edge_label_index[0,:]]\n",
        "            tail_embeddings = embeddings[target_edge_type[2]][edge_label_index[1,:]]\n",
        "\n",
        "\n",
        "        edgeindex = self.edgeindex_lookup[target_edge_type]\n",
        "        if get_head_fn=='loss':\n",
        "            loss = self.head.loss(head_embeddings, edgeindex.to(device), tail_embeddings, edge_label)\n",
        "            return loss\n",
        "        elif get_head_fn=='forward':\n",
        "            return self.head.forward(head_embeddings, edgeindex.to(device), tail_embeddings)\n",
        "\n",
        "\n",
        "metadata = train_data.metadata()\n",
        "# add selfloops\n",
        "for node_type in train_data.node_types:\n",
        "    metadata[1].append((node_type, 'self_loop', node_type))\n",
        "\n",
        "out_channels = 256\n",
        "hidden_channels = 256\n",
        "num_heads = 8\n",
        "num_layers = 2\n",
        "pnorm = 2\n",
        "head = 'TransE'\n",
        "gnn = HGT(hidden_channels=out_channels, out_channels=out_channels, num_heads=num_heads, num_layers=num_layers, node_types=train_data.node_types, data_metadata=metadata)\n",
        "\n",
        "model = Model(gnn, head=head, node_types=metadata[0], edge_types=metadata[1], ggn_output_dim=out_channels, pnorm=pnorm)\n",
        "#torch_geometric.compile(model, dynamic=True)\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4ebad4a8-bb51-45f9-8f93-8036fd9e89ae",
          "showTitle": false,
          "title": ""
        },
        "id": "uG7RfwhM6aMj"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "batch_size = 32\n",
        "num_node_types = len(train_data.node_types)\n",
        "print('num_node_types', num_node_types)\n",
        "one_hop_neighbors = (20 * batch_size)//num_node_types # per relationship type\n",
        "two_hop_neighbors = (20 * 8 * batch_size)//num_node_types # per relationship type\n",
        "three_hop_neighbors = (20 * 8 * 3 * batch_size)//num_node_types # per relationship type\n",
        "num_neighbors = [one_hop_neighbors, two_hop_neighbors] # three_hop_neighbors\n",
        "# num_neighbors [36, 363, 1454]\n",
        "\n",
        "print('num_neighbors', num_neighbors)\n",
        "print('avg_num_neighbors', [num_neighbors[0]/batch_size,num_neighbors[1]/batch_size,  num_neighbors[2]/batch_size if len(num_neighbors)==3 else 0 ])\n",
        "\n",
        "train_sampler = equal_edgeweight_hgt_sampler(train_data, batch_size, True, 'triplet', 1, num_neighbors, num_workers=0, prefetch_factor=None, pin_memory=True)\n",
        "val_sampler = equal_edgeweight_hgt_sampler(val_data, batch_size, True, 'triplet', 1, num_neighbors, num_workers=0, prefetch_factor=None, pin_memory=True)\n",
        "\n",
        "\n",
        "learning_rate = 2e-4\n",
        "# torch get optimizer by string name\n",
        "optimizer = 'Adam'\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #2e-15\n",
        "\n",
        "\n",
        "# create a tensorboard writer\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "neighbors = '_'.join([str(n) for n in num_neighbors])\n",
        "\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "writer = SummaryWriter(ROOT_FOLDER+f'runs/learningall_hgt_{timestamp}_margin05_pnorm{pnorm}_lr{learning_rate}_bs{batch_size}_neighbors_{neighbors}_head_{head}_hiddenchannels_{hidden_channels}_outchannels_{out_channels}_numheads_{num_heads}_numlayers_{num_layers}')\n",
        "print('writer',ROOT_FOLDER+f'runs/learningall_hgt_{timestamp}_margin05_pnorm{pnorm}_llr{learning_rate}_bs{batch_size}_neighbors_{neighbors}_head_{head}_hiddenchannels_{hidden_channels}_outchannels_{out_channels}_numheads_{num_heads}_numlayers_{num_layers}')\n",
        "\n",
        "model.train()\n",
        "start_epoch = 1\n",
        "total_minibatches = get_minibatch_count(train_data, batch_size)\n",
        "for epoch in range(start_epoch, start_epoch+1000):\n",
        "    for i, (same_nodetype, target_edge_type, minibatch) in tqdm(enumerate(train_sampler), total=total_minibatches):\n",
        "\n",
        "        try:\n",
        "            optimizer.zero_grad()\n",
        "            # batching is different depending on if node types in edge are same or different\n",
        "            print(target_edge_type)\n",
        "            if same_nodetype:\n",
        "\n",
        "                minibatch, edge_label_index, edge_label, input_edge_ids, global_node_ids = minibatch\n",
        "                #print(minibatch['jobs'].x.device, edge_label_index.device, edge_label.device)\n",
        "                loss = model(minibatch.to(device), target_edge_type, edge_label_index.to(device), edge_label.to(device))\n",
        "                #loss, pos, neg = model(minibatch, target_edge_type, edge_label_index, edge_label)\n",
        "            else:\n",
        "                try:\n",
        "                    minibatchpart1, minibatchpart2, edge_label_index, edge_label, input_edge_id, global_src_ids, global_dst_ids = minibatch\n",
        "                except ValueError as err:\n",
        "                    print('value error', err)\n",
        "                    continue # for skill qual edges sometimes for some reason only 5 instead of 7 elements returned\n",
        "                #print(minibatchpart1['jobs'].device, minibatchpart2['jobs'].device, edge_label_index.device, edge_label.device)\n",
        "                loss = model(minibatchpart1.to(device), target_edge_type, edge_label_index.to(device), edge_label.to(device), minibatchpart2.to(device))\n",
        "\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_samples_seen = i * batch_size\n",
        "            writer.add_scalar('Loss/train', loss.item(), total_samples_seen)\n",
        "\n",
        "            if i == total_minibatches-1:\n",
        "                print(f'{i} loss: {loss.item():.4f}')\n",
        "                writer.add_scalar('Epoch Loss/train', loss.item(), total_samples_seen)\n",
        "\n",
        "            # print loss and minibatch in the same line\n",
        "            print(f'{i} loss: {loss.item():.4f}', end='\\r')\n",
        "\n",
        "            if i % 300 == 0 or i == total_minibatches-1:\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    val_loss = 0\n",
        "                    for _ in range(3):\n",
        "                        try:\n",
        "                            same_nodetype, target_edge_type, minibatch = next(val_sampler)\n",
        "                        except StopIteration:\n",
        "                            val_sampler = iter(val_sampler)\n",
        "                            same_nodetype, target_edge_type, minibatch = next(val_sampler)\n",
        "\n",
        "                        if same_nodetype:\n",
        "                            minibatch, edge_label_index, edge_label, input_edge_ids, global_node_ids = minibatch\n",
        "                            #print(minibatch['jobs'].x.device, edge_label_index.device, edge_label.device)\n",
        "                            val_loss += model(minibatch.to(device), target_edge_type, edge_label_index.to(device), edge_label.to(device))\n",
        "                            #loss, pos, neg = model(minibatch, target_edge_type, edge_label_index, edge_label)\n",
        "                        else:\n",
        "                            try:\n",
        "                                minibatchpart1, minibatchpart2, edge_label_index, edge_label, input_edge_id, global_src_ids, global_dst_ids = minibatch\n",
        "                            except ValueError:\n",
        "                                continue\n",
        "\n",
        "                            #print(minibatchpart1['jobs'].device, minibatchpart2['jobs'].device, edge_label_index.device, edge_label.device)\n",
        "                            val_loss += model(minibatchpart1.to(device), target_edge_type, edge_label_index.to(device), edge_label.to(device), minibatchpart2.to(device))\n",
        "\n",
        "                val_loss /= 3\n",
        "                if i == 0:\n",
        "                    writer.add_scalar('Epoch Loss/val', val_loss, total_samples_seen)\n",
        "                    writer.add_scalar('Loss/val', val_loss, total_samples_seen)\n",
        "                elif i == total_minibatches-1:\n",
        "                    writer.add_scalar('Epoch Loss/val', val_loss, total_samples_seen)\n",
        "                else:\n",
        "                    writer.add_scalar('Loss/val', val_loss, total_samples_seen)\n",
        "\n",
        "\n",
        "                print(f'val_loss: {val_loss:.4f}', end='\\r')\n",
        "                model.train()\n",
        "\n",
        "            writer.flush()\n",
        "\n",
        "            if i % 1000 == 0 or i == total_minibatches-1:\n",
        "                folder = 'models'\n",
        "                if not os.path.exists(folder):\n",
        "                    os.makedirs(folder)\n",
        "\n",
        "                run_folder = ROOT_FOLDER+f'{folder}/learningall_hgt_{timestamp}_margin05_pnorm{pnorm}_llr{learning_rate}_bs{batch_size}_neighbors_{neighbors}_head_{head}_hiddenchannels_{hidden_channels}_outchannels_{out_channels}_numheads_{num_heads}_numlayers_{num_layers}'\n",
        "                if not os.path.exists(run_folder):\n",
        "                    os.makedirs(run_folder)\n",
        "\n",
        "                print('saving model to', run_folder)\n",
        "                # save model and optimizer\n",
        "                is_epoch = f'Ep{epoch}_' if i == total_minibatches-1 else ''\n",
        "                torch.save({\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    }, run_folder+f'/{is_epoch}model_samplesseen{total_samples_seen}.pt')\n",
        "\n",
        "        except IndexError:\n",
        "            print('indexerror')\n",
        "            pass\n",
        "\n",
        "writer.close()"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "learnings_training_v1_learning_unsupervised",
      "widgets": {}
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}